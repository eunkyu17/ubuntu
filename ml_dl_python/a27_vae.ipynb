{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33de06b4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    "  \"cells\": [\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": 20,\n",
    "      \"metadata\": {\n",
    "        \"colab\": {\n",
    "          \"base_uri\": \"https://localhost:8080/\"\n",
    "        },\n",
    "        \"id\": \"5XtyFSClpDlt\",\n",
    "        \"outputId\": \"b2746f8f-99f8-4151-8fa7-0833cab328e1\"\n",
    "      },\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"name\": \"stdout\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"cpu\\n\"\n",
    "          ]\n",
    "        }\n",
    "      ],\n",
    "      \"source\": [\n",
    "        \"import torch\\n\",\n",
    "        \"import torch.nn as nn\\n\",\n",
    "        \"import torch.optim as optim\\n\",\n",
    "        \"from torch.utils.data import DataLoader, DataLoader\\n\",\n",
    "        \"import matplotlib.pyplot as plt\\n\",\n",
    "        \"import numpy as np\\n\",\n",
    "        \"import pandas as pd\\n\",\n",
    "        \"import seaborn as sns\\n\",\n",
    "        \"import torchvision.datasets as datasets\\n\",\n",
    "        \"import torchvision.transforms as transforms\\n\",\n",
    "        \"\\n\",\n",
    "        \"device = torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\")\\n\",\n",
    "        \"print(device)\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": 21,\n",
    "      \"metadata\": {\n",
    "        \"id\": \"7vbl2f4upjus\"\n",
    "      },\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"name\": \"stdout\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\\n\",\n",
    "            \"Failed to download (trying next):\\n\",\n",
    "            \"HTTP Error 404: Not Found\\n\",\n",
    "            \"\\n\",\n",
    "            \"Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\\n\",\n",
    "            \"Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\\n\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stderr\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"100%|██████████| 9912422/9912422 [00:04<00:00, 2015600.42it/s]\\n\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stdout\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\\n\",\n",
    "            \"\\n\",\n",
    "            \"Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\\n\",\n",
    "            \"Failed to download (trying next):\\n\",\n",
    "            \"HTTP Error 404: Not Found\\n\",\n",
    "            \"\\n\",\n",
    "            \"Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\\n\",\n",
    "            \"Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\\n\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stderr\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"100%|██████████| 28881/28881 [00:00<00:00, 164680.52it/s]\\n\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stdout\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\\n\",\n",
    "            \"\\n\",\n",
    "            \"Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\\n\",\n",
    "            \"Failed to download (trying next):\\n\",\n",
    "            \"HTTP Error 404: Not Found\\n\",\n",
    "            \"\\n\",\n",
    "            \"Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\\n\",\n",
    "            \"Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\\n\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stderr\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"100%|██████████| 1648877/1648877 [00:01<00:00, 967852.33it/s] \\n\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stdout\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\\n\",\n",
    "            \"\\n\",\n",
    "            \"Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\\n\",\n",
    "            \"Failed to download (trying next):\\n\",\n",
    "            \"HTTP Error 404: Not Found\\n\",\n",
    "            \"\\n\",\n",
    "            \"Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\\n\",\n",
    "            \"Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\\n\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stderr\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"100%|██████████| 4542/4542 [00:00<00:00, 8218519.74it/s]\\n\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stdout\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\\n\",\n",
    "            \"\\n\"\n",
    "          ]\n",
    "        }\n",
    "      ],\n",
    "      \"source\": [\n",
    "        \"transform = transforms.Compose([transforms.ToTensor()])\\n\",\n",
    "        \"train_dataset = datasets.MNIST(\\n\",\n",
    "        \"    root=\\\"./data\\\", train=True, download=True, transform=transform\\n\",\n",
    "        \")\\n\",\n",
    "        \"test_dataset = datasets.MNIST(\\n\",\n",
    "        \"    root=\\\"./data\\\", train=False, download=True, transform=transform\\n\",\n",
    "        \")\\n\",\n",
    "        \"\\n\",\n",
    "        \"train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\\n\",\n",
    "        \"test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": 22,\n",
    "      \"metadata\": {\n",
    "        \"colab\": {\n",
    "          \"base_uri\": \"https://localhost:8080/\",\n",
    "          \"height\": 1000\n",
    "        },\n",
    "        \"id\": \"o0qappGWs96t\",\n",
    "        \"outputId\": \"414e74d1-30cf-4007-db66-68e906d20efc\"\n",
    "      },\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"name\": \"stdout\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"torch.Size([60000, 28, 28])\\n\",\n",
    "            \"tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\\n\",\n",
    "            \"           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\\n\",\n",
    "            \"        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\\n\",\n",
    "            \"           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\\n\",\n",
    "            \"        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\\n\",\n",
    "            \"           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\\n\",\n",
    "            \"        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\\n\",\n",
    "            \"           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\\n\",\n",
    "            \"        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\\n\",\n",
    "            \"           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\\n\",\n",
    "            \"        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,\\n\",\n",
    "            \"          18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0],\\n\",\n",
    "            \"        [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\\n\",\n",
    "            \"         253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,   0],\\n\",\n",
    "            \"        [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253, 253,\\n\",\n",
    "            \"         253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,   0,   0],\\n\",\n",
    "            \"        [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253, 253,\\n\",\n",
    "            \"         198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\\n\",\n",
    "            \"        [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253, 205,\\n\",\n",
    "            \"          11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\\n\",\n",
    "            \"        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,  90,\\n\",\n",
    "            \"           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\\n\",\n",
    "            \"        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253, 190,\\n\",\n",
    "            \"           2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\\n\",\n",
    "            \"        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,\\n\",\n",
    "            \"          70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\\n\",\n",
    "            \"        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35, 241,\\n\",\n",
    "            \"         225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\\n\",\n",
    "            \"        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  81,\\n\",\n",
    "            \"         240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,   0,   0],\\n\",\n",
    "            \"        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\\n\",\n",
    "            \"          45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,   0,   0],\\n\",\n",
    "            \"        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\\n\",\n",
    "            \"           0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,   0,   0],\\n\",\n",
    "            \"        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\\n\",\n",
    "            \"           0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,   0,   0],\\n\",\n",
    "            \"        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\\n\",\n",
    "            \"          46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0],\\n\",\n",
    "            \"        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\\n\",\n",
    "            \"         229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,   0],\\n\",\n",
    "            \"        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221, 253,\\n\",\n",
    "            \"         253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,   0,   0],\\n\",\n",
    "            \"        [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253, 253,\\n\",\n",
    "            \"         253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\\n\",\n",
    "            \"        [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253, 195,\\n\",\n",
    "            \"          80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\\n\",\n",
    "            \"        [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,  11,\\n\",\n",
    "            \"           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\\n\",\n",
    "            \"        [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,   0,\\n\",\n",
    "            \"           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\\n\",\n",
    "            \"        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\\n\",\n",
    "            \"           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\\n\",\n",
    "            \"        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\\n\",\n",
    "            \"           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\\n\",\n",
    "            \"        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\\n\",\n",
    "            \"           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\\n\",\n",
    "            \"       dtype=torch.uint8)\\n\",\n",
    "            \"tensor(5)\\n\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"data\": {\n",
    "            \"image/png\": \"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\",\n",
    "            \"text/plain\": [\n",
    "              \"<Figure size 640x480 with 1 Axes>\"\n",
    "            ]\n",
    "          },\n",
    "          \"metadata\": {},\n",
    "          \"output_type\": \"display_data\"\n",
    "        }\n",
    "      ],\n",
    "      \"source\": [\n",
    "        \"print(train_dataset.data.shape)\\n\",\n",
    "        \"# 첫번째 데이터 출력\\n\",\n",
    "        \"print(train_dataset.data[0])\\n\",\n",
    "        \"# matplot 으로 출력\\n\",\n",
    "        \"plt.imshow(train_dataset.data[0], cmap=\\\"gray\\\")\\n\",\n",
    "        \"print(train_dataset.targets[0])\\n\",\n",
    "        \"plt.show()\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": 23,\n",
    "      \"metadata\": {\n",
    "        \"id\": \"1Jze_V82qXkj\"\n",
    "      },\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"class Encoder(nn.Module):\\n\",\n",
    "        \"    def __init__(self, encoded_space_dim, fc2_input_dim):\\n\",\n",
    "        \"        super(Encoder, self).__init__()\\n\",\n",
    "        \"        self.encoder_cnn = nn.Sequential(\\n\",\n",
    "        \"            nn.Conv2d(1, 8, 3, stride=2, padding=1),\\n\",\n",
    "        \"            nn.ReLU(True),\\n\",\n",
    "        \"            nn.Conv2d(8, 16, 3, stride=2, padding=1),\\n\",\n",
    "        \"            nn.BatchNorm2d(16),\\n\",\n",
    "        \"            nn.ReLU(True),\\n\",\n",
    "        \"            nn.Conv2d(16, 32, 3, stride=2, padding=0),\\n\",\n",
    "        \"            nn.ReLU(True),\\n\",\n",
    "        \"        )\\n\",\n",
    "        \"        self.flatten = nn.Flatten(start_dim=1)\\n\",\n",
    "        \"        self.encoder_lin = nn.Sequential(\\n\",\n",
    "        \"            nn.Linear(3 * 3 * 32, 128), nn.ReLU(True), nn.Linear(128, encoded_space_dim)\\n\",\n",
    "        \"        )\\n\",\n",
    "        \"\\n\",\n",
    "        \"    def forward(self, x):\\n\",\n",
    "        \"        x = self.encoder_cnn(x)\\n\",\n",
    "        \"        x = self.flatten(x)\\n\",\n",
    "        \"        x = self.encoder_lin(x)\\n\",\n",
    "        \"        return x\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"class Decoder(nn.Module):\\n\",\n",
    "        \"    def __init__(self, encoded_space_dim, fc2_input_dim):\\n\",\n",
    "        \"        super(Decoder, self).__init__()\\n\",\n",
    "        \"        self.decoder_lin = nn.Sequential(\\n\",\n",
    "        \"            nn.Linear(encoded_space_dim, 128),\\n\",\n",
    "        \"            nn.ReLU(True),\\n\",\n",
    "        \"            nn.Linear(128, 3 * 3 * 32),\\n\",\n",
    "        \"            nn.ReLU(True),\\n\",\n",
    "        \"        )\\n\",\n",
    "        \"        self.unflatten = nn.Unflatten(dim=1, unflattened_size=(32, 3, 3))\\n\",\n",
    "        \"        self.decoder_conv = nn.Sequential(\\n\",\n",
    "        \"            nn.ConvTranspose2d(32, 16, 3, stride=2, output_padding=0),\\n\",\n",
    "        \"            nn.BatchNorm2d(16),\\n\",\n",
    "        \"            nn.ReLU(True),\\n\",\n",
    "        \"            nn.ConvTranspose2d(16, 8, 3, stride=2, padding=1, output_padding=1),\\n\",\n",
    "        \"            nn.BatchNorm2d(8),\\n\",\n",
    "        \"            nn.ReLU(True),\\n\",\n",
    "        \"            nn.ConvTranspose2d(8, 1, 3, stride=2, padding=1, output_padding=1),\\n\",\n",
    "        \"        )\\n\",\n",
    "        \"\\n\",\n",
    "        \"    def forward(self, x):\\n\",\n",
    "        \"        x = self.decoder_lin(x)\\n\",\n",
    "        \"        x = self.unflatten(x)\\n\",\n",
    "        \"        x = self.decoder_conv(x)\\n\",\n",
    "        \"        x = torch.sigmoid(x)\\n\",\n",
    "        \"        return x\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": 24,\n",
    "      \"metadata\": {\n",
    "        \"colab\": {\n",
    "          \"base_uri\": \"https://localhost:8080/\"\n",
    "        },\n",
    "        \"id\": \"deaTF5CwtrlX\",\n",
    "        \"outputId\": \"a13d21b1-ba48-4305-bfbe-b0ae9598a5e3\"\n",
    "      },\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"name\": \"stdout\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"Encoder(\\n\",\n",
    "            \"  (encoder_cnn): Sequential(\\n\",\n",
    "            \"    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\\n\",\n",
    "            \"    (1): ReLU(inplace=True)\\n\",\n",
    "            \"    (2): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\\n\",\n",
    "            \"    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\\n\",\n",
    "            \"    (4): ReLU(inplace=True)\\n\",\n",
    "            \"    (5): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2))\\n\",\n",
    "            \"    (6): ReLU(inplace=True)\\n\",\n",
    "            \"  )\\n\",\n",
    "            \"  (flatten): Flatten(start_dim=1, end_dim=-1)\\n\",\n",
    "            \"  (encoder_lin): Sequential(\\n\",\n",
    "            \"    (0): Linear(in_features=288, out_features=128, bias=True)\\n\",\n",
    "            \"    (1): ReLU(inplace=True)\\n\",\n",
    "            \"    (2): Linear(in_features=128, out_features=2, bias=True)\\n\",\n",
    "            \"  )\\n\",\n",
    "            \")\\n\"\n",
    "          ]\n",
    "        }\n",
    "      ],\n",
    "      \"source\": [\n",
    "        \"encoder = Encoder(encoded_space_dim=2, fc2_input_dim=128)\\n\",\n",
    "        \"decoder = Decoder(encoded_space_dim=2, fc2_input_dim=128)\\n\",\n",
    "        \"encoder.to(device)\\n\",\n",
    "        \"decoder.to(device)\\n\",\n",
    "        \"\\n\",\n",
    "        \"params_to_optimize = [\\n\",\n",
    "        \"    {\\\"params\\\": encoder.parameters()},\\n\",\n",
    "        \"    {\\\"params\\\": decoder.parameters()},\\n\",\n",
    "        \"]\\n\",\n",
    "        \"\\n\",\n",
    "        \"optimizer = optim.Adam(params_to_optimize, lr=0.001, weight_decay=1e-05)\\n\",\n",
    "        \"loss_fn = nn.MSELoss()\\n\",\n",
    "        \"print(encoder)\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": 25,\n",
    "      \"metadata\": {\n",
    "        \"id\": \"K0fO6gMkuPaq\"\n",
    "      },\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"def add_noise(inputs, noise_factor=0.3):\\n\",\n",
    "        \"    noisy = inputs + torch.randn_like(inputs) * noise_factor\\n\",\n",
    "        \"    noisy = torch.clip(noisy, 0.0, 1.0)\\n\",\n",
    "        \"    return noisy\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": 26,\n",
    "      \"metadata\": {\n",
    "        \"colab\": {\n",
    "          \"base_uri\": \"https://localhost:8080/\",\n",
    "          \"height\": 896\n",
    "        },\n",
    "        \"id\": \"LR3ryvpH33HJ\",\n",
    "        \"outputId\": \"f1d50638-3f3e-4196-8c33-730cff4f01f5\"\n",
    "      },\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"name\": \"stdout\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"torch.Size([28, 28])\\n\",\n",
    "            \"torch.Size([1, 28, 28])\\n\",\n",
    "            \"torch.Size([1, 28, 28])\\n\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"data\": {\n",
    "            \"image/png\": \"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\",\n",
    "            \"text/plain\": [\n",
    "              \"<Figure size 640x480 with 1 Axes>\"\n",
    "            ]\n",
    "          },\n",
    "          \"metadata\": {},\n",
    "          \"output_type\": \"display_data\"\n",
    "        },\n",
    "        {\n",
    "          \"data\": {\n",
    "            \"image/png\": \"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\",\n",
    "            \"text/plain\": [\n",
    "              \"<Figure size 640x480 with 1 Axes>\"\n",
    "            ]\n",
    "          },\n",
    "          \"metadata\": {},\n",
    "          \"output_type\": \"display_data\"\n",
    "        }\n",
    "      ],\n",
    "      \"source\": [\n",
    "        \"img = test_dataset.data[1].unsqueeze(0).float().to(device)\\n\",\n",
    "        \"print(test_dataset.data[1].shape)\\n\",\n",
    "        \"print(img.shape)\\n\",\n",
    "        \"noisy_img = add_noise(img, noise_factor=0.3).to(device)\\n\",\n",
    "        \"print(noisy_img.shape)\\n\",\n",
    "        \"plt.imshow(img.cpu().squeeze().numpy(), cmap=\\\"gray\\\")\\n\",\n",
    "        \"plt.show()\\n\",\n",
    "        \"plt.imshow(noisy_img.cpu().squeeze().numpy(), cmap=\\\"gray\\\")\\n\",\n",
    "        \"plt.show()\\n\",\n",
    "        \"rec_img = decoder(\\n\",\n",
    "        \"    encoder(test_dataset.data[0].float().to(device).reshape(1, 1, 28, 28))\\n\",\n",
    "        \")\\n\",\n",
    "        \"rec_img = decoder(encoder(noisy_img.reshape(1, 1, 28, 28)))\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": 27,\n",
    "      \"metadata\": {\n",
    "        \"id\": \"kZPtOQ4vuPdb\"\n",
    "      },\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"def plot_ae_outputs(encoder, decoder, n=15, noise_factor=0.3):\\n\",\n",
    "        \"    plt.figure(figsize=(10, 4.5))\\n\",\n",
    "        \"    for i in range(n):\\n\",\n",
    "        \"        ax = plt.subplot(3, n, i + 1)\\n\",\n",
    "        \"        img = test_dataset.data[i].unsqueeze(0).float().to(device)\\n\",\n",
    "        \"        noisy_img = add_noise(img, noise_factor).to(device)\\n\",\n",
    "        \"        encoder.eval()\\n\",\n",
    "        \"        decoder.eval()\\n\",\n",
    "        \"        with torch.no_grad():\\n\",\n",
    "        \"            rec_img = decoder(encoder(noisy_img.reshape(-1, 1, 28, 28)))\\n\",\n",
    "        \"        plt.imshow(img.cpu().squeeze().numpy(), cmap=\\\"gray\\\")\\n\",\n",
    "        \"        ax.get_xaxis().set_visible(False)\\n\",\n",
    "        \"        ax.get_yaxis().set_visible(False)\\n\",\n",
    "        \"        if i == n // 2:\\n\",\n",
    "        \"            ax.set_title(\\\"Original images\\\")\\n\",\n",
    "        \"        ax = plt.subplot(3, n, i + 1 + n)\\n\",\n",
    "        \"        plt.imshow(noisy_img.cpu().squeeze().numpy(), cmap=\\\"gray\\\")\\n\",\n",
    "        \"        ax.get_xaxis().set_visible(False)\\n\",\n",
    "        \"        ax.get_yaxis().set_visible(False)\\n\",\n",
    "        \"        if i == n // 2:\\n\",\n",
    "        \"            ax.set_title(\\\"Noisy Input\\\")\\n\",\n",
    "        \"\\n\",\n",
    "        \"        ax = plt.subplot(3, n, i + 1 + n * 2)\\n\",\n",
    "        \"        plt.imshow(rec_img.cpu().squeeze().numpy(), cmap=\\\"gray\\\")\\n\",\n",
    "        \"        ax.get_xaxis().set_visible(False)\\n\",\n",
    "        \"        ax.get_yaxis().set_visible(False)\\n\",\n",
    "        \"        if i == n // 2:\\n\",\n",
    "        \"            ax.set_title(\\\"Reconstructed Input\\\")\\n\",\n",
    "        \"        plt.subplots_adjust(\\n\",\n",
    "        \"            left=0.1, bottom=0.1, right=0.7, top=0.9, wspace=0.3, hspace=0.3\\n\",\n",
    "        \"        )\\n\",\n",
    "        \"    plt.show()\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": 28,\n",
    "      \"metadata\": {\n",
    "        \"id\": \"q5F-9x2P0Ubk\"\n",
    "      },\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"def train_epoch(\\n\",\n",
    "        \"    encoder, decoder, device, dataloader, loss_fn, optimizer, noise_factor=0.3\\n\",\n",
    "        \"):\\n\",\n",
    "        \"    encoder.train()\\n\",\n",
    "        \"    decoder.train()\\n\",\n",
    "        \"    train_loss = []\\n\",\n",
    "        \"    for image_batch, _ in dataloader:\\n\",\n",
    "        \"        image_batch = image_batch.to(device)\\n\",\n",
    "        \"        noisy_batch = add_noise(image_batch, noise_factor)\\n\",\n",
    "        \"        encoded_data = encoder(noisy_batch)\\n\",\n",
    "        \"        decoded_data = decoder(encoded_data)\\n\",\n",
    "        \"        loss = loss_fn(decoded_data, image_batch)\\n\",\n",
    "        \"        optimizer.zero_grad()\\n\",\n",
    "        \"        loss.backward()\\n\",\n",
    "        \"        optimizer.step()\\n\",\n",
    "        \"        train_loss.append(loss.detach().cpu().numpy())\\n\",\n",
    "        \"    return np.mean(train_loss)\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": 29,\n",
    "      \"metadata\": {\n",
    "        \"id\": \"UarA-_TV0UhZ\"\n",
    "      },\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"def test_epoch(encoder, decoder, device, dataloader, loss_fn, noise_factor=0.3):\\n\",\n",
    "        \"    encoder.eval()\\n\",\n",
    "        \"    decoder.eval()\\n\",\n",
    "        \"    with torch.no_grad():\\n\",\n",
    "        \"        conc_out = []\\n\",\n",
    "        \"        conc_label = []\\n\",\n",
    "        \"        for image_batch, _ in dataloader:\\n\",\n",
    "        \"            image_batch = image_batch.to(device)\\n\",\n",
    "        \"            noisy_batch = add_noise(image_batch, noise_factor)\\n\",\n",
    "        \"            encoded_data = encoder(noisy_batch)\\n\",\n",
    "        \"            decoded_data = decoder(encoded_data)\\n\",\n",
    "        \"            conc_out.append(decoded_data.cpu())\\n\",\n",
    "        \"            conc_label.append(image_batch.cpu())\\n\",\n",
    "        \"        conc_out = torch.cat(conc_out)\\n\",\n",
    "        \"        conc_label = torch.cat(conc_label)\\n\",\n",
    "        \"        val_loss = loss_fn(conc_out, conc_label)\\n\",\n",
    "        \"    return val_loss.data\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": 30,\n",
    "      \"metadata\": {\n",
    "        \"colab\": {\n",
    "          \"base_uri\": \"https://localhost:8080/\",\n",
    "          \"height\": 1000\n",
    "        },\n",
    "        \"id\": \"IuKbq73KzKHx\",\n",
    "        \"outputId\": \"396598cf-d38b-4f28-97ba-0135d7b3217a\"\n",
    "      },\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"name\": \"stdout\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"EPOCHE 1/30\\n\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"ename\": \"KeyboardInterrupt\",\n",
    "          \"evalue\": \"\",\n",
    "          \"output_type\": \"error\",\n",
    "          \"traceback\": [\n",
    "            \"\\u001b[0;31m---------------------------------------------------------------------------\\u001b[0m\",\n",
    "            \"\\u001b[0;31mKeyboardInterrupt\\u001b[0m                         Traceback (most recent call last)\",\n",
    "            \"Cell \\u001b[0;32mIn[30], line 6\\u001b[0m\\n\\u001b[1;32m      4\\u001b[0m \\u001b[38;5;28;01mfor\\u001b[39;00m epoch \\u001b[38;5;129;01min\\u001b[39;00m \\u001b[38;5;28mrange\\u001b[39m(num_epochs):\\n\\u001b[1;32m      5\\u001b[0m     \\u001b[38;5;28mprint\\u001b[39m(\\u001b[38;5;124m\\\"\\u001b[39m\\u001b[38;5;124mEPOCHE \\u001b[39m\\u001b[38;5;132;01m%d\\u001b[39;00m\\u001b[38;5;124m/\\u001b[39m\\u001b[38;5;132;01m%d\\u001b[39;00m\\u001b[38;5;124m\\\"\\u001b[39m \\u001b[38;5;241m%\\u001b[39m (epoch \\u001b[38;5;241m+\\u001b[39m \\u001b[38;5;241m1\\u001b[39m, num_epochs))\\n\\u001b[0;32m----> 6\\u001b[0m     train_loss \\u001b[38;5;241m=\\u001b[39m \\u001b[43mtrain_epoch\\u001b[49m\\u001b[43m(\\u001b[49m\\n\\u001b[1;32m      7\\u001b[0m \\u001b[43m        \\u001b[49m\\u001b[43mencoder\\u001b[49m\\u001b[38;5;241;43m=\\u001b[39;49m\\u001b[43mencoder\\u001b[49m\\u001b[43m,\\u001b[49m\\n\\u001b[1;32m      8\\u001b[0m \\u001b[43m        \\u001b[49m\\u001b[43mdecoder\\u001b[49m\\u001b[38;5;241;43m=\\u001b[39;49m\\u001b[43mdecoder\\u001b[49m\\u001b[43m,\\u001b[49m\\n\\u001b[1;32m      9\\u001b[0m \\u001b[43m        \\u001b[49m\\u001b[43mdevice\\u001b[49m\\u001b[38;5;241;43m=\\u001b[39;49m\\u001b[43mdevice\\u001b[49m\\u001b[43m,\\u001b[49m\\n\\u001b[1;32m     10\\u001b[0m \\u001b[43m        \\u001b[49m\\u001b[43mdataloader\\u001b[49m\\u001b[38;5;241;43m=\\u001b[39;49m\\u001b[43mtrain_loader\\u001b[49m\\u001b[43m,\\u001b[49m\\n\\u001b[1;32m     11\\u001b[0m \\u001b[43m        \\u001b[49m\\u001b[43mloss_fn\\u001b[49m\\u001b[38;5;241;43m=\\u001b[39;49m\\u001b[43mloss_fn\\u001b[49m\\u001b[43m,\\u001b[49m\\n\\u001b[1;32m     12\\u001b[0m \\u001b[43m        \\u001b[49m\\u001b[43moptimizer\\u001b[49m\\u001b[38;5;241;43m=\\u001b[39;49m\\u001b[43moptimizer\\u001b[49m\\u001b[43m,\\u001b[49m\\n\\u001b[1;32m     13\\u001b[0m \\u001b[43m        \\u001b[49m\\u001b[43mnoise_factor\\u001b[49m\\u001b[38;5;241;43m=\\u001b[39;49m\\u001b[38;5;241;43m0.3\\u001b[39;49m\\u001b[43m,\\u001b[49m\\n\\u001b[1;32m     14\\u001b[0m \\u001b[43m    \\u001b[49m\\u001b[43m)\\u001b[49m\\n\\u001b[1;32m     15\\u001b[0m     val_loss \\u001b[38;5;241m=\\u001b[39m test_epoch(\\n\\u001b[1;32m     16\\u001b[0m         encoder\\u001b[38;5;241m=\\u001b[39mencoder,\\n\\u001b[1;32m     17\\u001b[0m         decoder\\u001b[38;5;241m=\\u001b[39mdecoder,\\n\\u001b[0;32m   (...)\\u001b[0m\\n\\u001b[1;32m     21\\u001b[0m         noise_factor\\u001b[38;5;241m=\\u001b[39m\\u001b[38;5;241m0.3\\u001b[39m,\\n\\u001b[1;32m     22\\u001b[0m     )\\n\\u001b[1;32m     23\\u001b[0m     history_da[\\u001b[38;5;124m\\\"\\u001b[39m\\u001b[38;5;124mtrain_loss\\u001b[39m\\u001b[38;5;124m\\\"\\u001b[39m]\\u001b[38;5;241m.\\u001b[39mappend(train_loss)\\n\",\n",
    "            \"Cell \\u001b[0;32mIn[28], line 10\\u001b[0m, in \\u001b[0;36mtrain_epoch\\u001b[0;34m(encoder, decoder, device, dataloader, loss_fn, optimizer, noise_factor)\\u001b[0m\\n\\u001b[1;32m      8\\u001b[0m image_batch \\u001b[38;5;241m=\\u001b[39m image_batch\\u001b[38;5;241m.\\u001b[39mto(device)\\n\\u001b[1;32m      9\\u001b[0m noisy_batch \\u001b[38;5;241m=\\u001b[39m add_noise(image_batch, noise_factor)\\n\\u001b[0;32m---> 10\\u001b[0m encoded_data \\u001b[38;5;241m=\\u001b[39m \\u001b[43mencoder\\u001b[49m\\u001b[43m(\\u001b[49m\\u001b[43mnoisy_batch\\u001b[49m\\u001b[43m)\\u001b[49m\\n\\u001b[1;32m     11\\u001b[0m decoded_data \\u001b[38;5;241m=\\u001b[39m decoder(encoded_data)\\n\\u001b[1;32m     12\\u001b[0m loss \\u001b[38;5;241m=\\u001b[39m loss_fn(decoded_data, image_batch)\\n\",\n",
    "            \"File \\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\\u001b[0m, in \\u001b[0;36mModule._wrapped_call_impl\\u001b[0;34m(self, *args, **kwargs)\\u001b[0m\\n\\u001b[1;32m   1509\\u001b[0m     \\u001b[38;5;28;01mreturn\\u001b[39;00m \\u001b[38;5;28mself\\u001b[39m\\u001b[38;5;241m.\\u001b[39m_compiled_call_impl(\\u001b[38;5;241m*\\u001b[39margs, \\u001b[38;5;241m*\\u001b[39m\\u001b[38;5;241m*\\u001b[39mkwargs)  \\u001b[38;5;66;03m# type: ignore[misc]\\u001b[39;00m\\n\\u001b[1;32m   1510\\u001b[0m \\u001b[38;5;28;01melse\\u001b[39;00m:\\n\\u001b[0;32m-> 1511\\u001b[0m     \\u001b[38;5;28;01mreturn\\u001b[39;00m \\u001b[38;5;28;43mself\\u001b[39;49m\\u001b[38;5;241;43m.\\u001b[39;49m\\u001b[43m_call_impl\\u001b[49m\\u001b[43m(\\u001b[49m\\u001b[38;5;241;43m*\\u001b[39;49m\\u001b[43margs\\u001b[49m\\u001b[43m,\\u001b[49m\\u001b[43m \\u001b[49m\\u001b[38;5;241;43m*\\u001b[39;49m\\u001b[38;5;241;43m*\\u001b[39;49m\\u001b[43mkwargs\\u001b[49m\\u001b[43m)\\u001b[49m\\n\",\n",
    "            \"File \\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\\u001b[0m, in \\u001b[0;36mModule._call_impl\\u001b[0;34m(self, *args, **kwargs)\\u001b[0m\\n\\u001b[1;32m   1515\\u001b[0m \\u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\\u001b[39;00m\\n\\u001b[1;32m   1516\\u001b[0m \\u001b[38;5;66;03m# this function, and just call forward.\\u001b[39;00m\\n\\u001b[1;32m   1517\\u001b[0m \\u001b[38;5;28;01mif\\u001b[39;00m \\u001b[38;5;129;01mnot\\u001b[39;00m (\\u001b[38;5;28mself\\u001b[39m\\u001b[38;5;241m.\\u001b[39m_backward_hooks \\u001b[38;5;129;01mor\\u001b[39;00m \\u001b[38;5;28mself\\u001b[39m\\u001b[38;5;241m.\\u001b[39m_backward_pre_hooks \\u001b[38;5;129;01mor\\u001b[39;00m \\u001b[38;5;28mself\\u001b[39m\\u001b[38;5;241m.\\u001b[39m_forward_hooks \\u001b[38;5;129;01mor\\u001b[39;00m \\u001b[38;5;28mself\\u001b[39m\\u001b[38;5;241m.\\u001b[39m_forward_pre_hooks\\n\\u001b[1;32m   1518\\u001b[0m         \\u001b[38;5;129;01mor\\u001b[39;00m _global_backward_pre_hooks \\u001b[38;5;129;01mor\\u001b[39;00m _global_backward_hooks\\n\\u001b[1;32m   1519\\u001b[0m         \\u001b[38;5;129;01mor\\u001b[39;00m _global_forward_hooks \\u001b[38;5;129;01mor\\u001b[39;00m _global_forward_pre_hooks):\\n\\u001b[0;32m-> 1520\\u001b[0m     \\u001b[38;5;28;01mreturn\\u001b[39;00m \\u001b[43mforward_call\\u001b[49m\\u001b[43m(\\u001b[49m\\u001b[38;5;241;43m*\\u001b[39;49m\\u001b[43margs\\u001b[49m\\u001b[43m,\\u001b[49m\\u001b[43m \\u001b[49m\\u001b[38;5;241;43m*\\u001b[39;49m\\u001b[38;5;241;43m*\\u001b[39;49m\\u001b[43mkwargs\\u001b[49m\\u001b[43m)\\u001b[49m\\n\\u001b[1;32m   1522\\u001b[0m \\u001b[38;5;28;01mtry\\u001b[39;00m:\\n\\u001b[1;32m   1523\\u001b[0m     result \\u001b[38;5;241m=\\u001b[39m \\u001b[38;5;28;01mNone\\u001b[39;00m\\n\",\n",
    "            \"Cell \\u001b[0;32mIn[23], line 19\\u001b[0m, in \\u001b[0;36mEncoder.forward\\u001b[0;34m(self, x)\\u001b[0m\\n\\u001b[1;32m     18\\u001b[0m \\u001b[38;5;28;01mdef\\u001b[39;00m \\u001b[38;5;21mforward\\u001b[39m(\\u001b[38;5;28mself\\u001b[39m, x):\\n\\u001b[0;32m---> 19\\u001b[0m     x \\u001b[38;5;241m=\\u001b[39m \\u001b[38;5;28;43mself\\u001b[39;49m\\u001b[38;5;241;43m.\\u001b[39;49m\\u001b[43mencoder_cnn\\u001b[49m\\u001b[43m(\\u001b[49m\\u001b[43mx\\u001b[49m\\u001b[43m)\\u001b[49m\\n\\u001b[1;32m     20\\u001b[0m     x \\u001b[38;5;241m=\\u001b[39m \\u001b[38;5;28mself\\u001b[39m\\u001b[38;5;241m.\\u001b[39mflatten(x)\\n\\u001b[1;32m     21\\u001b[0m     x \\u001b[38;5;241m=\\u001b[39m \\u001b[38;5;28mself\\u001b[39m\\u001b[38;5;241m.\\u001b[39mencoder_lin(x)\\n\",\n",
    "            \"File \\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\\u001b[0m, in \\u001b[0;36mModule._wrapped_call_impl\\u001b[0;34m(self, *args, **kwargs)\\u001b[0m\\n\\u001b[1;32m   1509\\u001b[0m     \\u001b[38;5;28;01mreturn\\u001b[39;00m \\u001b[38;5;28mself\\u001b[39m\\u001b[38;5;241m.\\u001b[39m_compiled_call_impl(\\u001b[38;5;241m*\\u001b[39margs, \\u001b[38;5;241m*\\u001b[39m\\u001b[38;5;241m*\\u001b[39mkwargs)  \\u001b[38;5;66;03m# type: ignore[misc]\\u001b[39;00m\\n\\u001b[1;32m   1510\\u001b[0m \\u001b[38;5;28;01melse\\u001b[39;00m:\\n\\u001b[0;32m-> 1511\\u001b[0m     \\u001b[38;5;28;01mreturn\\u001b[39;00m \\u001b[38;5;28;43mself\\u001b[39;49m\\u001b[38;5;241;43m.\\u001b[39;49m\\u001b[43m_call_impl\\u001b[49m\\u001b[43m(\\u001b[49m\\u001b[38;5;241;43m*\\u001b[39;49m\\u001b[43margs\\u001b[49m\\u001b[43m,\\u001b[49m\\u001b[43m \\u001b[49m\\u001b[38;5;241;43m*\\u001b[39;49m\\u001b[38;5;241;43m*\\u001b[39;49m\\u001b[43mkwargs\\u001b[49m\\u001b[43m)\\u001b[49m\\n\",\n",
    "            \"File \\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\\u001b[0m, in \\u001b[0;36mModule._call_impl\\u001b[0;34m(self, *args, **kwargs)\\u001b[0m\\n\\u001b[1;32m   1515\\u001b[0m \\u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\\u001b[39;00m\\n\\u001b[1;32m   1516\\u001b[0m \\u001b[38;5;66;03m# this function, and just call forward.\\u001b[39;00m\\n\\u001b[1;32m   1517\\u001b[0m \\u001b[38;5;28;01mif\\u001b[39;00m \\u001b[38;5;129;01mnot\\u001b[39;00m (\\u001b[38;5;28mself\\u001b[39m\\u001b[38;5;241m.\\u001b[39m_backward_hooks \\u001b[38;5;129;01mor\\u001b[39;00m \\u001b[38;5;28mself\\u001b[39m\\u001b[38;5;241m.\\u001b[39m_backward_pre_hooks \\u001b[38;5;129;01mor\\u001b[39;00m \\u001b[38;5;28mself\\u001b[39m\\u001b[38;5;241m.\\u001b[39m_forward_hooks \\u001b[38;5;129;01mor\\u001b[39;00m \\u001b[38;5;28mself\\u001b[39m\\u001b[38;5;241m.\\u001b[39m_forward_pre_hooks\\n\\u001b[1;32m   1518\\u001b[0m         \\u001b[38;5;129;01mor\\u001b[39;00m _global_backward_pre_hooks \\u001b[38;5;129;01mor\\u001b[39;00m _global_backward_hooks\\n\\u001b[1;32m   1519\\u001b[0m         \\u001b[38;5;129;01mor\\u001b[39;00m _global_forward_hooks \\u001b[38;5;129;01mor\\u001b[39;00m _global_forward_pre_hooks):\\n\\u001b[0;32m-> 1520\\u001b[0m     \\u001b[38;5;28;01mreturn\\u001b[39;00m \\u001b[43mforward_call\\u001b[49m\\u001b[43m(\\u001b[49m\\u001b[38;5;241;43m*\\u001b[39;49m\\u001b[43margs\\u001b[49m\\u001b[43m,\\u001b[49m\\u001b[43m \\u001b[49m\\u001b[38;5;241;43m*\\u001b[39;49m\\u001b[38;5;241;43m*\\u001b[39;49m\\u001b[43mkwargs\\u001b[49m\\u001b[43m)\\u001b[49m\\n\\u001b[1;32m   1522\\u001b[0m \\u001b[38;5;28;01mtry\\u001b[39;00m:\\n\\u001b[1;32m   1523\\u001b[0m     result \\u001b[38;5;241m=\\u001b[39m \\u001b[38;5;28;01mNone\\u001b[39;00m\\n\",\n",
    "            \"File \\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:217\\u001b[0m, in \\u001b[0;36mSequential.forward\\u001b[0;34m(self, input)\\u001b[0m\\n\\u001b[1;32m    215\\u001b[0m \\u001b[38;5;28;01mdef\\u001b[39;00m \\u001b[38;5;21mforward\\u001b[39m(\\u001b[38;5;28mself\\u001b[39m, \\u001b[38;5;28minput\\u001b[39m):\\n\\u001b[1;32m    216\\u001b[0m     \\u001b[38;5;28;01mfor\\u001b[39;00m module \\u001b[38;5;129;01min\\u001b[39;00m \\u001b[38;5;28mself\\u001b[39m:\\n\\u001b[0;32m--> 217\\u001b[0m         \\u001b[38;5;28minput\\u001b[39m \\u001b[38;5;241m=\\u001b[39m \\u001b[43mmodule\\u001b[49m\\u001b[43m(\\u001b[49m\\u001b[38;5;28;43minput\\u001b[39;49m\\u001b[43m)\\u001b[49m\\n\\u001b[1;32m    218\\u001b[0m     \\u001b[38;5;28;01mreturn\\u001b[39;00m \\u001b[38;5;28minput\\u001b[39m\\n\",\n",
    "            \"File \\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\\u001b[0m, in \\u001b[0;36mModule._wrapped_call_impl\\u001b[0;34m(self, *args, **kwargs)\\u001b[0m\\n\\u001b[1;32m   1509\\u001b[0m     \\u001b[38;5;28;01mreturn\\u001b[39;00m \\u001b[38;5;28mself\\u001b[39m\\u001b[38;5;241m.\\u001b[39m_compiled_call_impl(\\u001b[38;5;241m*\\u001b[39margs, \\u001b[38;5;241m*\\u001b[39m\\u001b[38;5;241m*\\u001b[39mkwargs)  \\u001b[38;5;66;03m# type: ignore[misc]\\u001b[39;00m\\n\\u001b[1;32m   1510\\u001b[0m \\u001b[38;5;28;01melse\\u001b[39;00m:\\n\\u001b[0;32m-> 1511\\u001b[0m     \\u001b[38;5;28;01mreturn\\u001b[39;00m \\u001b[38;5;28;43mself\\u001b[39;49m\\u001b[38;5;241;43m.\\u001b[39;49m\\u001b[43m_call_impl\\u001b[49m\\u001b[43m(\\u001b[49m\\u001b[38;5;241;43m*\\u001b[39;49m\\u001b[43margs\\u001b[49m\\u001b[43m,\\u001b[49m\\u001b[43m \\u001b[49m\\u001b[38;5;241;43m*\\u001b[39;49m\\u001b[38;5;241;43m*\\u001b[39;49m\\u001b[43mkwargs\\u001b[49m\\u001b[43m)\\u001b[49m\\n\",\n",
    "            \"File \\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\\u001b[0m, in \\u001b[0;36mModule._call_impl\\u001b[0;34m(self, *args, **kwargs)\\u001b[0m\\n\\u001b[1;32m   1515\\u001b[0m \\u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\\u001b[39;00m\\n\\u001b[1;32m   1516\\u001b[0m \\u001b[38;5;66;03m# this function, and just call forward.\\u001b[39;00m\\n\\u001b[1;32m   1517\\u001b[0m \\u001b[38;5;28;01mif\\u001b[39;00m \\u001b[38;5;129;01mnot\\u001b[39;00m (\\u001b[38;5;28mself\\u001b[39m\\u001b[38;5;241m.\\u001b[39m_backward_hooks \\u001b[38;5;129;01mor\\u001b[39;00m \\u001b[38;5;28mself\\u001b[39m\\u001b[38;5;241m.\\u001b[39m_backward_pre_hooks \\u001b[38;5;129;01mor\\u001b[39;00m \\u001b[38;5;28mself\\u001b[39m\\u001b[38;5;241m.\\u001b[39m_forward_hooks \\u001b[38;5;129;01mor\\u001b[39;00m \\u001b[38;5;28mself\\u001b[39m\\u001b[38;5;241m.\\u001b[39m_forward_pre_hooks\\n\\u001b[1;32m   1518\\u001b[0m         \\u001b[38;5;129;01mor\\u001b[39;00m _global_backward_pre_hooks \\u001b[38;5;129;01mor\\u001b[39;00m _global_backward_hooks\\n\\u001b[1;32m   1519\\u001b[0m         \\u001b[38;5;129;01mor\\u001b[39;00m _global_forward_hooks \\u001b[38;5;129;01mor\\u001b[39;00m _global_forward_pre_hooks):\\n\\u001b[0;32m-> 1520\\u001b[0m     \\u001b[38;5;28;01mreturn\\u001b[39;00m \\u001b[43mforward_call\\u001b[49m\\u001b[43m(\\u001b[49m\\u001b[38;5;241;43m*\\u001b[39;49m\\u001b[43margs\\u001b[49m\\u001b[43m,\\u001b[49m\\u001b[43m \\u001b[49m\\u001b[38;5;241;43m*\\u001b[39;49m\\u001b[38;5;241;43m*\\u001b[39;49m\\u001b[43mkwargs\\u001b[49m\\u001b[43m)\\u001b[49m\\n\\u001b[1;32m   1522\\u001b[0m \\u001b[38;5;28;01mtry\\u001b[39;00m:\\n\\u001b[1;32m   1523\\u001b[0m     result \\u001b[38;5;241m=\\u001b[39m \\u001b[38;5;28;01mNone\\u001b[39;00m\\n\",\n",
    "            \"File \\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\\u001b[0m, in \\u001b[0;36mConv2d.forward\\u001b[0;34m(self, input)\\u001b[0m\\n\\u001b[1;32m    459\\u001b[0m \\u001b[38;5;28;01mdef\\u001b[39;00m \\u001b[38;5;21mforward\\u001b[39m(\\u001b[38;5;28mself\\u001b[39m, \\u001b[38;5;28minput\\u001b[39m: Tensor) \\u001b[38;5;241m-\\u001b[39m\\u001b[38;5;241m>\\u001b[39m Tensor:\\n\\u001b[0;32m--> 460\\u001b[0m     \\u001b[38;5;28;01mreturn\\u001b[39;00m \\u001b[38;5;28;43mself\\u001b[39;49m\\u001b[38;5;241;43m.\\u001b[39;49m\\u001b[43m_conv_forward\\u001b[49m\\u001b[43m(\\u001b[49m\\u001b[38;5;28;43minput\\u001b[39;49m\\u001b[43m,\\u001b[49m\\u001b[43m \\u001b[49m\\u001b[38;5;28;43mself\\u001b[39;49m\\u001b[38;5;241;43m.\\u001b[39;49m\\u001b[43mweight\\u001b[49m\\u001b[43m,\\u001b[49m\\u001b[43m \\u001b[49m\\u001b[38;5;28;43mself\\u001b[39;49m\\u001b[38;5;241;43m.\\u001b[39;49m\\u001b[43mbias\\u001b[49m\\u001b[43m)\\u001b[49m\\n\",\n",
    "            \"File \\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\\u001b[0m, in \\u001b[0;36mConv2d._conv_forward\\u001b[0;34m(self, input, weight, bias)\\u001b[0m\\n\\u001b[1;32m    452\\u001b[0m \\u001b[38;5;28;01mif\\u001b[39;00m \\u001b[38;5;28mself\\u001b[39m\\u001b[38;5;241m.\\u001b[39mpadding_mode \\u001b[38;5;241m!=\\u001b[39m \\u001b[38;5;124m'\\u001b[39m\\u001b[38;5;124mzeros\\u001b[39m\\u001b[38;5;124m'\\u001b[39m:\\n\\u001b[1;32m    453\\u001b[0m     \\u001b[38;5;28;01mreturn\\u001b[39;00m F\\u001b[38;5;241m.\\u001b[39mconv2d(F\\u001b[38;5;241m.\\u001b[39mpad(\\u001b[38;5;28minput\\u001b[39m, \\u001b[38;5;28mself\\u001b[39m\\u001b[38;5;241m.\\u001b[39m_reversed_padding_repeated_twice, mode\\u001b[38;5;241m=\\u001b[39m\\u001b[38;5;28mself\\u001b[39m\\u001b[38;5;241m.\\u001b[39mpadding_mode),\\n\\u001b[1;32m    454\\u001b[0m                     weight, bias, \\u001b[38;5;28mself\\u001b[39m\\u001b[38;5;241m.\\u001b[39mstride,\\n\\u001b[1;32m    455\\u001b[0m                     _pair(\\u001b[38;5;241m0\\u001b[39m), \\u001b[38;5;28mself\\u001b[39m\\u001b[38;5;241m.\\u001b[39mdilation, \\u001b[38;5;28mself\\u001b[39m\\u001b[38;5;241m.\\u001b[39mgroups)\\n\\u001b[0;32m--> 456\\u001b[0m \\u001b[38;5;28;01mreturn\\u001b[39;00m \\u001b[43mF\\u001b[49m\\u001b[38;5;241;43m.\\u001b[39;49m\\u001b[43mconv2d\\u001b[49m\\u001b[43m(\\u001b[49m\\u001b[38;5;28;43minput\\u001b[39;49m\\u001b[43m,\\u001b[49m\\u001b[43m \\u001b[49m\\u001b[43mweight\\u001b[49m\\u001b[43m,\\u001b[49m\\u001b[43m \\u001b[49m\\u001b[43mbias\\u001b[49m\\u001b[43m,\\u001b[49m\\u001b[43m \\u001b[49m\\u001b[38;5;28;43mself\\u001b[39;49m\\u001b[38;5;241;43m.\\u001b[39;49m\\u001b[43mstride\\u001b[49m\\u001b[43m,\\u001b[49m\\n\\u001b[1;32m    457\\u001b[0m \\u001b[43m                \\u001b[49m\\u001b[38;5;28;43mself\\u001b[39;49m\\u001b[38;5;241;43m.\\u001b[39;49m\\u001b[43mpadding\\u001b[49m\\u001b[43m,\\u001b[49m\\u001b[43m \\u001b[49m\\u001b[38;5;28;43mself\\u001b[39;49m\\u001b[38;5;241;43m.\\u001b[39;49m\\u001b[43mdilation\\u001b[49m\\u001b[43m,\\u001b[49m\\u001b[43m \\u001b[49m\\u001b[38;5;28;43mself\\u001b[39;49m\\u001b[38;5;241;43m.\\u001b[39;49m\\u001b[43mgroups\\u001b[49m\\u001b[43m)\\u001b[49m\\n\",\n",
    "            \"\\u001b[0;31mKeyboardInterrupt\\u001b[0m: \"\n",
    "          ]\n",
    "        }\n",
    "      ],\n",
    "      \"source\": [\n",
    "        \"num_epochs = 30\\n\",\n",
    "        \"history_da = {\\\"train_loss\\\": [], \\\"test_loss\\\": []}\\n\",\n",
    "        \"loss_fn = nn.MSELoss()\\n\",\n",
    "        \"for epoch in range(num_epochs):\\n\",\n",
    "        \"    print(\\\"EPOCHE %d/%d\\\" % (epoch + 1, num_epochs))\\n\",\n",
    "        \"    train_loss = train_epoch(\\n\",\n",
    "        \"        encoder=encoder,\\n\",\n",
    "        \"        decoder=decoder,\\n\",\n",
    "        \"        device=device,\\n\",\n",
    "        \"        dataloader=train_loader,\\n\",\n",
    "        \"        loss_fn=loss_fn,\\n\",\n",
    "        \"        optimizer=optimizer,\\n\",\n",
    "        \"        noise_factor=0.3,\\n\",\n",
    "        \"    )\\n\",\n",
    "        \"    val_loss = test_epoch(\\n\",\n",
    "        \"        encoder=encoder,\\n\",\n",
    "        \"        decoder=decoder,\\n\",\n",
    "        \"        device=device,\\n\",\n",
    "        \"        dataloader=test_loader,\\n\",\n",
    "        \"        loss_fn=loss_fn,\\n\",\n",
    "        \"        noise_factor=0.3,\\n\",\n",
    "        \"    )\\n\",\n",
    "        \"    history_da[\\\"train_loss\\\"].append(train_loss)\\n\",\n",
    "        \"    history_da[\\\"test_loss\\\"].append(val_loss)\\n\",\n",
    "        \"    print(\\n\",\n",
    "        \"        \\\"\\\\n EPOCH {}/{} \\\\t train loss {:.3f} \\\\t val loss {:.3f}\\\".format(\\n\",\n",
    "        \"            epoch + 1, num_epochs, train_loss, val_loss\\n\",\n",
    "        \"        )\\n\",\n",
    "        \"    )\\n\",\n",
    "        \"    plot_ae_outputs(encoder=encoder, decoder=decoder, noise_factor=0.3)\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {\n",
    "        \"id\": \"fSzXXbjDzKU9\"\n",
    "      },\n",
    "      \"outputs\": [],\n",
    "      \"source\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {\n",
    "        \"colab\": {\n",
    "          \"base_uri\": \"https://localhost:8080/\",\n",
    "          \"height\": 448\n",
    "        },\n",
    "        \"id\": \"jyU3RUcU82Cq\",\n",
    "        \"outputId\": \"4979975e-97ff-47b6-d19d-79412305d8f3\"\n",
    "      },\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"name\": \"stdout\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"[0.7362131865085972, -0.8336742868210263]\\n\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"data\": {\n",
    "            \"image/png\": \"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\",\n",
    "            \"text/plain\": [\n",
    "              \"<Figure size 640x480 with 1 Axes>\"\n",
    "            ]\n",
    "          },\n",
    "          \"metadata\": {},\n",
    "          \"output_type\": \"display_data\"\n",
    "        }\n",
    "      ],\n",
    "      \"source\": [\n",
    "        \"# 임의의 숫자를 넣어서 random 으로 decoder 만으로 숫자 그림 생성하기\\n\",\n",
    "        \"import random\\n\",\n",
    "        \"\\n\",\n",
    "        \"random_data = [random.random() * 2 - 1 for _ in range(2)]\\n\",\n",
    "        \"# random_data = [0.337, 0.49]\\n\",\n",
    "        \"print(random_data)\\n\",\n",
    "        \"output = decoder(torch.Tensor(random_data).to(device).reshape(-1, 2))\\n\",\n",
    "        \"plt.imshow(output.cpu().squeeze().detach().numpy(), cmap=\\\"gray\\\")\\n\",\n",
    "        \"plt.show()\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {\n",
    "        \"colab\": {\n",
    "          \"base_uri\": \"https://localhost:8080/\"\n",
    "        },\n",
    "        \"id\": \"7D3Orc4m86Yo\",\n",
    "        \"outputId\": \"fd718ae3-6e28-4c4b-8028-bd34d4a32937\"\n",
    "      },\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"name\": \"stdout\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"tensor(5)\\n\",\n",
    "            \"tensor([[-0.2937, -0.0568]], device='cuda:0', grad_fn=<AddmmBackward0>)\\n\",\n",
    "            \"tensor(0)\\n\",\n",
    "            \"tensor([[-1.4202,  1.4741]], device='cuda:0', grad_fn=<AddmmBackward0>)\\n\",\n",
    "            \"tensor(4)\\n\",\n",
    "            \"tensor([[0.3951, 1.1770]], device='cuda:0', grad_fn=<AddmmBackward0>)\\n\",\n",
    "            \"tensor(1)\\n\",\n",
    "            \"tensor([[ 0.1161, -0.1165]], device='cuda:0', grad_fn=<AddmmBackward0>)\\n\",\n",
    "            \"tensor(9)\\n\",\n",
    "            \"tensor([[0.3552, 0.4243]], device='cuda:0', grad_fn=<AddmmBackward0>)\\n\",\n",
    "            \"tensor(2)\\n\",\n",
    "            \"tensor([[ 0.1076, -1.0921]], device='cuda:0', grad_fn=<AddmmBackward0>)\\n\",\n",
    "            \"tensor(1)\\n\",\n",
    "            \"tensor([[ 0.4377, -0.0127]], device='cuda:0', grad_fn=<AddmmBackward0>)\\n\",\n",
    "            \"tensor(3)\\n\",\n",
    "            \"tensor([[-0.2711, -0.0906]], device='cuda:0', grad_fn=<AddmmBackward0>)\\n\",\n",
    "            \"tensor(1)\\n\",\n",
    "            \"tensor([[ 0.4554, -0.0114]], device='cuda:0', grad_fn=<AddmmBackward0>)\\n\",\n",
    "            \"tensor(4)\\n\",\n",
    "            \"tensor([[0.1645, 0.1385]], device='cuda:0', grad_fn=<AddmmBackward0>)\\n\"\n",
    "          ]\n",
    "        }\n",
    "      ],\n",
    "      \"source\": [\n",
    "        \"# train data 로 숫자 얻기\\n\",\n",
    "        \"for i in range(10):\\n\",\n",
    "        \"    output = encoder(train_dataset.data[i].float().to(device).reshape(1, 1, 28, 28))\\n\",\n",
    "        \"    print(train_dataset.targets[i])\\n\",\n",
    "        \"    print(output)\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {\n",
    "        \"id\": \"CgMhvYXK_MEa\"\n",
    "      },\n",
    "      \"outputs\": [],\n",
    "      \"source\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"O6CV6hPwBbK5\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"## VAE\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": 31,\n",
    "      \"metadata\": {\n",
    "        \"colab\": {\n",
    "          \"base_uri\": \"https://localhost:8080/\"\n",
    "        },\n",
    "        \"id\": \"-BIqbh4tBcvx\",\n",
    "        \"outputId\": \"5e00625d-7523-4541-e481-c75f20b6d085\"\n",
    "      },\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"name\": \"stdout\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"cpu\\n\"\n",
    "          ]\n",
    "        }\n",
    "      ],\n",
    "      \"source\": [\n",
    "        \"import torch\\n\",\n",
    "        \"import torch.nn as nn\\n\",\n",
    "        \"import torch.optim as optim\\n\",\n",
    "        \"from torch.utils.data import DataLoader, DataLoader\\n\",\n",
    "        \"import matplotlib.pyplot as plt\\n\",\n",
    "        \"import numpy as np\\n\",\n",
    "        \"import pandas as pd\\n\",\n",
    "        \"import seaborn as sns\\n\",\n",
    "        \"import torchvision.datasets as datasets\\n\",\n",
    "        \"import torchvision.transforms as transforms\\n\",\n",
    "        \"\\n\",\n",
    "        \"device = torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\")\\n\",\n",
    "        \"print(device)\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": 32,\n",
    "      \"metadata\": {\n",
    "        \"id\": \"NXvjArDKBfbo\"\n",
    "      },\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"transform = transforms.Compose([transforms.ToTensor()])\\n\",\n",
    "        \"train_dataset = datasets.MNIST(\\n\",\n",
    "        \"    root=\\\"./data\\\", train=True, download=True, transform=transform\\n\",\n",
    "        \")\\n\",\n",
    "        \"test_dataset = datasets.MNIST(\\n\",\n",
    "        \"    root=\\\"./data\\\", train=False, download=True, transform=transform\\n\",\n",
    "        \")\\n\",\n",
    "        \"\\n\",\n",
    "        \"train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True, pin_memory=False)\\n\",\n",
    "        \"test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False)\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": 33,\n",
    "      \"metadata\": {\n",
    "        \"id\": \"pBsN4hU2Bv1-\"\n",
    "      },\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"class Encoder(nn.Module):\\n\",\n",
    "        \"    def __init__(self, input_dim, hidden_dim, latent_dim):\\n\",\n",
    "        \"        super(Encoder, self).__init__()\\n\",\n",
    "        \"        self.input1 = nn.Linear(input_dim, hidden_dim)\\n\",\n",
    "        \"        self.input2 = nn.Linear(hidden_dim, hidden_dim)\\n\",\n",
    "        \"        self.mean = nn.Linear(hidden_dim, latent_dim)\\n\",\n",
    "        \"        self.var = nn.Linear(hidden_dim, latent_dim)\\n\",\n",
    "        \"        self.LeakyRelu = nn.LeakyReLU(0.2)\\n\",\n",
    "        \"        self.training = True\\n\",\n",
    "        \"\\n\",\n",
    "        \"    def forward(self, x):\\n\",\n",
    "        \"        h_ = self.LeakyRelu(self.input1(x))\\n\",\n",
    "        \"        h_ = self.LeakyRelu(self.input2(h_))\\n\",\n",
    "        \"        mean = self.mean(h_)\\n\",\n",
    "        \"        log_var = self.var(h_)\\n\",\n",
    "        \"        return mean, log_var\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"class Decoder(nn.Module):\\n\",\n",
    "        \"    def __init__(self, latent_dim, hidden_dim, output_dim):\\n\",\n",
    "        \"        super(Decoder, self).__init__()\\n\",\n",
    "        \"        self.hidden1 = nn.Linear(latent_dim, hidden_dim)\\n\",\n",
    "        \"        self.hidden2 = nn.Linear(hidden_dim, hidden_dim)\\n\",\n",
    "        \"        self.output = nn.Linear(hidden_dim, output_dim)\\n\",\n",
    "        \"        self.LeakyRelu = nn.LeakyReLU(0.2)\\n\",\n",
    "        \"\\n\",\n",
    "        \"    def forward(self, x):\\n\",\n",
    "        \"        h = self.LeakyRelu(self.hidden1(x))\\n\",\n",
    "        \"        h = self.LeakyRelu(self.hidden2(h))\\n\",\n",
    "        \"        return torch.sigmoid(self.output(h))\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"class Model(nn.Module):\\n\",\n",
    "        \"    def __init__(self, Encoder, Decoder):\\n\",\n",
    "        \"        super(Model, self).__init__()\\n\",\n",
    "        \"        self.Encoder = Encoder\\n\",\n",
    "        \"        self.Decoder = Decoder\\n\",\n",
    "        \"\\n\",\n",
    "        \"    def reparameterization(self, mean, var):\\n\",\n",
    "        \"        epsilon = torch.randn_like(var).to(device)\\n\",\n",
    "        \"        z = mean + var * epsilon\\n\",\n",
    "        \"        return z\\n\",\n",
    "        \"\\n\",\n",
    "        \"    def forward(self, x):\\n\",\n",
    "        \"        mean, log_var = self.Encoder(x)\\n\",\n",
    "        \"        z = self.reparameterization(mean, torch.exp(0.5 * log_var))\\n\",\n",
    "        \"        x_hat = self.Decoder(z)\\n\",\n",
    "        \"        return x_hat, mean, log_var\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": 34,\n",
    "      \"metadata\": {\n",
    "        \"id\": \"d163mlyTELMP\"\n",
    "      },\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"x_dim = 784\\n\",\n",
    "        \"hidden_dim = 400\\n\",\n",
    "        \"latent_dim = 200\\n\",\n",
    "        \"epochs = 30\\n\",\n",
    "        \"batch_size = 100\\n\",\n",
    "        \"\\n\",\n",
    "        \"encoder = Encoder(input_dim=x_dim, hidden_dim=hidden_dim, latent_dim=latent_dim)\\n\",\n",
    "        \"decoder = Decoder(latent_dim=latent_dim, hidden_dim=hidden_dim, output_dim=x_dim)\\n\",\n",
    "        \"model = Model(Encoder=encoder, Decoder=decoder).to(device)\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": 40,\n",
    "      \"metadata\": {\n",
    "        \"id\": \"Uqp5mTrGEeey\"\n",
    "      },\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"def loss_function(x, x_hat, mean, log_var):\\n\",\n",
    "        \"    reproduction_loss = nn.functional.binary_cross_entropy(x_hat, x, reduction=\\\"sum\\\")\\n\",\n",
    "        \"    KLD = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\\n\",\n",
    "        \"    return reproduction_loss, KLD\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": 36,\n",
    "      \"metadata\": {\n",
    "        \"colab\": {\n",
    "          \"base_uri\": \"https://localhost:8080/\"\n",
    "        },\n",
    "        \"id\": \"hAfK0HUfEvQ2\",\n",
    "        \"outputId\": \"0d5c1014-8423-4e40-9bef-82ae8c25600e\"\n",
    "      },\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"name\": \"stdout\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"Defaulting to user installation because normal site-packages is not writeable\\n\",\n",
    "            \"Requirement already satisfied: tensorboardX in /home/aa/.local/lib/python3.10/site-packages (2.6.4)\\n\",\n",
    "            \"Requirement already satisfied: numpy in /home/aa/.local/lib/python3.10/site-packages (from tensorboardX) (1.26.4)\\n\",\n",
    "            \"Requirement already satisfied: protobuf>=3.20 in /home/aa/.local/lib/python3.10/site-packages (from tensorboardX) (5.29.5)\\n\",\n",
    "            \"Requirement already satisfied: packaging in /home/aa/.local/lib/python3.10/site-packages (from tensorboardX) (25.0)\\n\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"data\": {\n",
    "            \"text/plain\": [\n",
    "              \"Model(\\n\",\n",
    "              \"  (Encoder): Encoder(\\n\",\n",
    "              \"    (input1): Linear(in_features=784, out_features=400, bias=True)\\n\",\n",
    "              \"    (input2): Linear(in_features=400, out_features=400, bias=True)\\n\",\n",
    "              \"    (mean): Linear(in_features=400, out_features=200, bias=True)\\n\",\n",
    "              \"    (var): Linear(in_features=400, out_features=200, bias=True)\\n\",\n",
    "              \"    (LeakyRelu): LeakyReLU(negative_slope=0.2)\\n\",\n",
    "              \"  )\\n\",\n",
    "              \"  (Decoder): Decoder(\\n\",\n",
    "              \"    (hidden1): Linear(in_features=200, out_features=400, bias=True)\\n\",\n",
    "              \"    (hidden2): Linear(in_features=400, out_features=400, bias=True)\\n\",\n",
    "              \"    (output): Linear(in_features=400, out_features=784, bias=True)\\n\",\n",
    "              \"    (LeakyRelu): LeakyReLU(negative_slope=0.2)\\n\",\n",
    "              \"  )\\n\",\n",
    "              \")\"\n",
    "            ]\n",
    "          },\n",
    "          \"execution_count\": 36,\n",
    "          \"metadata\": {},\n",
    "          \"output_type\": \"execute_result\"\n",
    "        }\n",
    "      ],\n",
    "      \"source\": [\n",
    "        \"\\n\",\n",
    "        \"!pip install tensorboardX\\n\",\n",
    "        \"# 6 model train function\\n\",\n",
    "        \"from tensorboardX import SummaryWriter\\n\",\n",
    "        \"saved_loc = \\\"scalar/\\\"\\n\",\n",
    "        \"writer = SummaryWriter(saved_loc)\\n\",\n",
    "        \"\\n\",\n",
    "        \"model.train()\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": 37,\n",
    "      \"metadata\": {\n",
    "        \"id\": \"qMt2mdyAFBFV\"\n",
    "      },\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"model.train()\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"def train(epoch, model, train_loder, optimizer):\\n\",\n",
    "        \"    train_loss = 0\\n\",\n",
    "        \"    for batch_idx, (x, _) in enumerate(train_loder):\\n\",\n",
    "        \"        x = x.view(batch_size, x_dim)\\n\",\n",
    "        \"        x = x.to(device)\\n\",\n",
    "        \"\\n\",\n",
    "        \"        optimizer.zero_grad()\\n\",\n",
    "        \"        x_hat, mean, log_var = model(x)\\n\",\n",
    "        \"        BCE, KLD = loss_function(x, x_hat, mean, log_var)\\n\",\n",
    "        \"        loss = BCE + KLD\\n\",\n",
    "        \"        writer.add_scalar(\\n\",\n",
    "        \"            \\\"Train/Reconstruction Error\\\",\\n\",\n",
    "        \"            BCE.item(),\\n\",\n",
    "        \"            batch_idx + epoch * len(train_loder.dataset) / batch_size,\\n\",\n",
    "        \"        )\\n\",\n",
    "        \"        writer.add_scalar(\\n\",\n",
    "        \"            \\\"Train/KL-Divergence\\\",\\n\",\n",
    "        \"            KLD.item(),\\n\",\n",
    "        \"            batch_idx + epoch * len(train_loder.dataset) / batch_size,\\n\",\n",
    "        \"        )\\n\",\n",
    "        \"        writer.add_scalar(\\n\",\n",
    "        \"            \\\"Train/Total Loss\\\",\\n\",\n",
    "        \"            loss.item(),\\n\",\n",
    "        \"            batch_idx + epoch * len(train_loder.dataset) / batch_size,\\n\",\n",
    "        \"        )\\n\",\n",
    "        \"        train_loss += loss.item()\\n\",\n",
    "        \"        loss.backward()\\n\",\n",
    "        \"        optimizer.step()\\n\",\n",
    "        \"\\n\",\n",
    "        \"        if batch_idx % 100 == 0:\\n\",\n",
    "        \"            print(\\n\",\n",
    "        \"                f\\\"Train Epoch: {epoch} [{batch_idx * len(x)}/{len(train_loder.dataset)} ({100. * batch_idx / len(train_loder):.0f}%)]\\\\tLoss: {loss.item() / len(x):.6f}\\\"\\n\",\n",
    "        \"            )\\n\",\n",
    "        \"    print(\\n\",\n",
    "        \"        \\\"====> Epoch: {} Average loss: {:.4f}\\\".format(\\n\",\n",
    "        \"            epoch, train_loss / len(train_loder.dataset)\\n\",\n",
    "        \"        )\\n\",\n",
    "        \"    )\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": 38,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"import torchvision\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"# 7 model test function\\n\",\n",
    "        \"def test(epoch, model, test_loader):\\n\",\n",
    "        \"    model.eval()\\n\",\n",
    "        \"    test_loss = 0\\n\",\n",
    "        \"    with torch.no_grad():\\n\",\n",
    "        \"        for batch_idx, (x, _) in enumerate(test_loader):\\n\",\n",
    "        \"            x = x.view(batch_size, x_dim)\\n\",\n",
    "        \"            x = x.to(device)\\n\",\n",
    "        \"            x_hat, mean, log_var = model(x)\\n\",\n",
    "        \"            BCE, KLD = loss_function(x, x_hat, mean, log_var)\\n\",\n",
    "        \"            loss = BCE + KLD\\n\",\n",
    "        \"\\n\",\n",
    "        \"            writer.add_scalar(\\n\",\n",
    "        \"                \\\"Test/Reconstruction Error\\\",\\n\",\n",
    "        \"                BCE.item(),\\n\",\n",
    "        \"                batch_idx + epoch * len(test_loader.dataset) / batch_size,\\n\",\n",
    "        \"            )\\n\",\n",
    "        \"            writer.add_scalar(\\n\",\n",
    "        \"                \\\"Test/KL-Divergence\\\",\\n\",\n",
    "        \"                KLD.item(),\\n\",\n",
    "        \"                batch_idx + epoch * len(test_loader.dataset) / batch_size,\\n\",\n",
    "        \"            )\\n\",\n",
    "        \"            writer.add_scalar(\\n\",\n",
    "        \"                \\\"Test/Total Loss\\\",\\n\",\n",
    "        \"                loss.item(),\\n\",\n",
    "        \"                batch_idx + epoch * len(test_loader.dataset) / batch_size,\\n\",\n",
    "        \"            )\\n\",\n",
    "        \"            test_loss += loss.item()\\n\",\n",
    "        \"\\n\",\n",
    "        \"            if batch_idx == 0:\\n\",\n",
    "        \"                n = min(x.size(0), 8)\\n\",\n",
    "        \"                comparison = torch.cat([x[:n], x_hat.view(batch_size, x_dim)[:n]])\\n\",\n",
    "        \"                grid = torchvision.utils.make_grid(comparison.cpu())\\n\",\n",
    "        \"                writer.add_image(\\n\",\n",
    "        \"                    \\\"Test image - Above: real data, below: reconstructed data\\\",\\n\",\n",
    "        \"                    grid,\\n\",\n",
    "        \"                    epoch,\\n\",\n",
    "        \"                )\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": 41,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"name\": \"stderr\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"  0%|          | 0/30 [00:00<?, ?it/s]\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stdout\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"Train Epoch: 0 [0/60000 (0%)]\\tLoss: 546.614570\\n\",\n",
    "            \"Train Epoch: 0 [10000/60000 (17%)]\\tLoss: 198.424434\\n\",\n",
    "            \"Train Epoch: 0 [20000/60000 (33%)]\\tLoss: 181.682344\\n\",\n",
    "            \"Train Epoch: 0 [30000/60000 (50%)]\\tLoss: 167.997051\\n\",\n",
    "            \"Train Epoch: 0 [40000/60000 (67%)]\\tLoss: 162.484590\\n\",\n",
    "            \"Train Epoch: 0 [50000/60000 (83%)]\\tLoss: 153.794434\\n\",\n",
    "            \"====> Epoch: 0 Average loss: 173.8586\\n\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stderr\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"  3%|▎         | 1/30 [00:40<19:39, 40.66s/it]\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stdout\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"\\n\",\n",
    "            \"\\n\",\n",
    "            \"Train Epoch: 1 [0/60000 (0%)]\\tLoss: 139.436836\\n\",\n",
    "            \"Train Epoch: 1 [10000/60000 (17%)]\\tLoss: 135.349912\\n\",\n",
    "            \"Train Epoch: 1 [20000/60000 (33%)]\\tLoss: 135.653252\\n\",\n",
    "            \"Train Epoch: 1 [30000/60000 (50%)]\\tLoss: 129.021875\\n\",\n",
    "            \"Train Epoch: 1 [40000/60000 (67%)]\\tLoss: 126.412813\\n\",\n",
    "            \"Train Epoch: 1 [50000/60000 (83%)]\\tLoss: 124.715244\\n\",\n",
    "            \"====> Epoch: 1 Average loss: 128.6077\\n\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stderr\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"  7%|▋         | 2/30 [01:07<15:16, 32.73s/it]\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stdout\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"\\n\",\n",
    "            \"\\n\",\n",
    "            \"Train Epoch: 2 [0/60000 (0%)]\\tLoss: 116.183799\\n\",\n",
    "            \"Train Epoch: 2 [10000/60000 (17%)]\\tLoss: 122.979102\\n\",\n",
    "            \"Train Epoch: 2 [20000/60000 (33%)]\\tLoss: 113.985059\\n\",\n",
    "            \"Train Epoch: 2 [30000/60000 (50%)]\\tLoss: 109.813096\\n\",\n",
    "            \"Train Epoch: 2 [40000/60000 (67%)]\\tLoss: 117.770859\\n\",\n",
    "            \"Train Epoch: 2 [50000/60000 (83%)]\\tLoss: 114.689688\\n\",\n",
    "            \"====> Epoch: 2 Average loss: 116.4751\\n\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stderr\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \" 10%|█         | 3/30 [01:30<12:36, 28.02s/it]\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stdout\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"\\n\",\n",
    "            \"\\n\",\n",
    "            \"Train Epoch: 3 [0/60000 (0%)]\\tLoss: 115.305654\\n\",\n",
    "            \"Train Epoch: 3 [10000/60000 (17%)]\\tLoss: 115.133369\\n\",\n",
    "            \"Train Epoch: 3 [20000/60000 (33%)]\\tLoss: 111.037871\\n\",\n",
    "            \"Train Epoch: 3 [30000/60000 (50%)]\\tLoss: 105.408701\\n\",\n",
    "            \"Train Epoch: 3 [40000/60000 (67%)]\\tLoss: 109.589736\\n\",\n",
    "            \"Train Epoch: 3 [50000/60000 (83%)]\\tLoss: 112.611465\\n\",\n",
    "            \"====> Epoch: 3 Average loss: 112.2880\\n\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stderr\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \" 13%|█▎        | 4/30 [01:55<11:35, 26.76s/it]\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stdout\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"\\n\",\n",
    "            \"\\n\",\n",
    "            \"Train Epoch: 4 [0/60000 (0%)]\\tLoss: 109.241650\\n\",\n",
    "            \"Train Epoch: 4 [10000/60000 (17%)]\\tLoss: 119.196836\\n\",\n",
    "            \"Train Epoch: 4 [20000/60000 (33%)]\\tLoss: 112.791592\\n\",\n",
    "            \"Train Epoch: 4 [30000/60000 (50%)]\\tLoss: 113.264395\\n\",\n",
    "            \"Train Epoch: 4 [40000/60000 (67%)]\\tLoss: 110.370254\\n\",\n",
    "            \"Train Epoch: 4 [50000/60000 (83%)]\\tLoss: 103.955723\\n\",\n",
    "            \"====> Epoch: 4 Average loss: 110.1213\\n\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stderr\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \" 17%|█▋        | 5/30 [02:22<11:19, 27.18s/it]\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stdout\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"\\n\",\n",
    "            \"\\n\",\n",
    "            \"Train Epoch: 5 [0/60000 (0%)]\\tLoss: 112.547168\\n\",\n",
    "            \"Train Epoch: 5 [10000/60000 (17%)]\\tLoss: 111.574678\\n\",\n",
    "            \"Train Epoch: 5 [20000/60000 (33%)]\\tLoss: 114.872490\\n\",\n",
    "            \"Train Epoch: 5 [30000/60000 (50%)]\\tLoss: 105.986426\\n\",\n",
    "            \"Train Epoch: 5 [40000/60000 (67%)]\\tLoss: 107.145752\\n\",\n",
    "            \"Train Epoch: 5 [50000/60000 (83%)]\\tLoss: 108.526211\\n\",\n",
    "            \"====> Epoch: 5 Average loss: 108.4643\\n\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stderr\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \" 20%|██        | 6/30 [02:49<10:49, 27.06s/it]\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stdout\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"\\n\",\n",
    "            \"\\n\",\n",
    "            \"Train Epoch: 6 [0/60000 (0%)]\\tLoss: 109.284434\\n\",\n",
    "            \"Train Epoch: 6 [10000/60000 (17%)]\\tLoss: 109.701055\\n\",\n",
    "            \"Train Epoch: 6 [20000/60000 (33%)]\\tLoss: 106.186934\\n\",\n",
    "            \"Train Epoch: 6 [30000/60000 (50%)]\\tLoss: 110.025225\\n\",\n",
    "            \"Train Epoch: 6 [40000/60000 (67%)]\\tLoss: 106.295928\\n\",\n",
    "            \"Train Epoch: 6 [50000/60000 (83%)]\\tLoss: 110.582803\\n\",\n",
    "            \"====> Epoch: 6 Average loss: 107.2100\\n\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stderr\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \" 23%|██▎       | 7/30 [03:14<10:06, 26.38s/it]\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stdout\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"\\n\",\n",
    "            \"\\n\",\n",
    "            \"Train Epoch: 7 [0/60000 (0%)]\\tLoss: 106.009707\\n\",\n",
    "            \"Train Epoch: 7 [10000/60000 (17%)]\\tLoss: 108.095273\\n\",\n",
    "            \"Train Epoch: 7 [20000/60000 (33%)]\\tLoss: 105.279072\\n\",\n",
    "            \"Train Epoch: 7 [30000/60000 (50%)]\\tLoss: 103.934092\\n\",\n",
    "            \"Train Epoch: 7 [40000/60000 (67%)]\\tLoss: 101.900361\\n\",\n",
    "            \"Train Epoch: 7 [50000/60000 (83%)]\\tLoss: 110.775195\\n\",\n",
    "            \"====> Epoch: 7 Average loss: 106.4089\\n\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stderr\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \" 27%|██▋       | 8/30 [03:44<10:00, 27.30s/it]\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stdout\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"\\n\",\n",
    "            \"\\n\",\n",
    "            \"Train Epoch: 8 [0/60000 (0%)]\\tLoss: 110.345781\\n\",\n",
    "            \"Train Epoch: 8 [10000/60000 (17%)]\\tLoss: 110.849316\\n\",\n",
    "            \"Train Epoch: 8 [20000/60000 (33%)]\\tLoss: 105.728467\\n\",\n",
    "            \"Train Epoch: 8 [30000/60000 (50%)]\\tLoss: 106.667070\\n\",\n",
    "            \"Train Epoch: 8 [40000/60000 (67%)]\\tLoss: 106.891191\\n\",\n",
    "            \"Train Epoch: 8 [50000/60000 (83%)]\\tLoss: 104.814600\\n\",\n",
    "            \"====> Epoch: 8 Average loss: 105.6458\\n\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stderr\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \" 30%|███       | 9/30 [04:11<09:31, 27.21s/it]\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stdout\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"\\n\",\n",
    "            \"\\n\",\n",
    "            \"Train Epoch: 9 [0/60000 (0%)]\\tLoss: 105.477246\\n\",\n",
    "            \"Train Epoch: 9 [10000/60000 (17%)]\\tLoss: 108.557842\\n\",\n",
    "            \"Train Epoch: 9 [20000/60000 (33%)]\\tLoss: 100.475117\\n\",\n",
    "            \"Train Epoch: 9 [30000/60000 (50%)]\\tLoss: 106.744414\\n\",\n",
    "            \"Train Epoch: 9 [40000/60000 (67%)]\\tLoss: 104.830586\\n\",\n",
    "            \"Train Epoch: 9 [50000/60000 (83%)]\\tLoss: 104.472334\\n\",\n",
    "            \"====> Epoch: 9 Average loss: 104.9430\\n\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stderr\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \" 33%|███▎      | 10/30 [04:35<08:47, 26.35s/it]\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stdout\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"\\n\",\n",
    "            \"\\n\",\n",
    "            \"Train Epoch: 10 [0/60000 (0%)]\\tLoss: 102.550195\\n\",\n",
    "            \"Train Epoch: 10 [10000/60000 (17%)]\\tLoss: 102.738887\\n\",\n",
    "            \"Train Epoch: 10 [20000/60000 (33%)]\\tLoss: 109.628711\\n\",\n",
    "            \"Train Epoch: 10 [30000/60000 (50%)]\\tLoss: 102.444883\\n\",\n",
    "            \"Train Epoch: 10 [40000/60000 (67%)]\\tLoss: 105.425518\\n\",\n",
    "            \"Train Epoch: 10 [50000/60000 (83%)]\\tLoss: 101.848594\\n\",\n",
    "            \"====> Epoch: 10 Average loss: 104.3815\\n\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stderr\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \" 37%|███▋      | 11/30 [04:57<07:56, 25.06s/it]\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stdout\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"\\n\",\n",
    "            \"\\n\",\n",
    "            \"Train Epoch: 11 [0/60000 (0%)]\\tLoss: 104.867471\\n\",\n",
    "            \"Train Epoch: 11 [10000/60000 (17%)]\\tLoss: 105.518008\\n\",\n",
    "            \"Train Epoch: 11 [20000/60000 (33%)]\\tLoss: 107.845957\\n\",\n",
    "            \"Train Epoch: 11 [30000/60000 (50%)]\\tLoss: 104.812832\\n\",\n",
    "            \"Train Epoch: 11 [40000/60000 (67%)]\\tLoss: 105.701191\\n\",\n",
    "            \"Train Epoch: 11 [50000/60000 (83%)]\\tLoss: 95.891504\\n\",\n",
    "            \"====> Epoch: 11 Average loss: 103.9301\\n\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stderr\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \" 40%|████      | 12/30 [05:18<07:10, 23.89s/it]\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stdout\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"\\n\",\n",
    "            \"\\n\",\n",
    "            \"Train Epoch: 12 [0/60000 (0%)]\\tLoss: 103.282656\\n\",\n",
    "            \"Train Epoch: 12 [10000/60000 (17%)]\\tLoss: 102.218828\\n\",\n",
    "            \"Train Epoch: 12 [20000/60000 (33%)]\\tLoss: 101.538730\\n\",\n",
    "            \"Train Epoch: 12 [30000/60000 (50%)]\\tLoss: 99.848184\\n\",\n",
    "            \"Train Epoch: 12 [40000/60000 (67%)]\\tLoss: 98.414873\\n\",\n",
    "            \"Train Epoch: 12 [50000/60000 (83%)]\\tLoss: 98.870830\\n\",\n",
    "            \"====> Epoch: 12 Average loss: 103.4782\\n\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stderr\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \" 43%|████▎     | 13/30 [05:40<06:35, 23.28s/it]\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stdout\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"\\n\",\n",
    "            \"\\n\",\n",
    "            \"Train Epoch: 13 [0/60000 (0%)]\\tLoss: 106.234209\\n\",\n",
    "            \"Train Epoch: 13 [10000/60000 (17%)]\\tLoss: 103.496787\\n\",\n",
    "            \"Train Epoch: 13 [20000/60000 (33%)]\\tLoss: 99.929521\\n\",\n",
    "            \"Train Epoch: 13 [30000/60000 (50%)]\\tLoss: 110.121543\\n\",\n",
    "            \"Train Epoch: 13 [40000/60000 (67%)]\\tLoss: 99.724365\\n\",\n",
    "            \"Train Epoch: 13 [50000/60000 (83%)]\\tLoss: 103.819863\\n\",\n",
    "            \"====> Epoch: 13 Average loss: 103.1343\\n\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stderr\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \" 47%|████▋     | 14/30 [06:02<06:06, 22.93s/it]\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stdout\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"\\n\",\n",
    "            \"\\n\",\n",
    "            \"Train Epoch: 14 [0/60000 (0%)]\\tLoss: 101.223281\\n\",\n",
    "            \"Train Epoch: 14 [10000/60000 (17%)]\\tLoss: 105.098516\\n\",\n",
    "            \"Train Epoch: 14 [20000/60000 (33%)]\\tLoss: 101.510479\\n\",\n",
    "            \"Train Epoch: 14 [30000/60000 (50%)]\\tLoss: 105.316279\\n\",\n",
    "            \"Train Epoch: 14 [40000/60000 (67%)]\\tLoss: 103.708701\\n\",\n",
    "            \"Train Epoch: 14 [50000/60000 (83%)]\\tLoss: 102.260937\\n\",\n",
    "            \"====> Epoch: 14 Average loss: 102.9189\\n\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stderr\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \" 50%|█████     | 15/30 [06:24<05:38, 22.55s/it]\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stdout\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"\\n\",\n",
    "            \"\\n\",\n",
    "            \"Train Epoch: 15 [0/60000 (0%)]\\tLoss: 101.701211\\n\",\n",
    "            \"Train Epoch: 15 [10000/60000 (17%)]\\tLoss: 101.388496\\n\",\n",
    "            \"Train Epoch: 15 [20000/60000 (33%)]\\tLoss: 99.044922\\n\",\n",
    "            \"Train Epoch: 15 [30000/60000 (50%)]\\tLoss: 100.867666\\n\",\n",
    "            \"Train Epoch: 15 [40000/60000 (67%)]\\tLoss: 98.173320\\n\",\n",
    "            \"Train Epoch: 15 [50000/60000 (83%)]\\tLoss: 100.842363\\n\",\n",
    "            \"====> Epoch: 15 Average loss: 102.5690\\n\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stderr\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \" 53%|█████▎    | 16/30 [06:46<05:11, 22.24s/it]\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stdout\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"\\n\",\n",
    "            \"\\n\",\n",
    "            \"Train Epoch: 16 [0/60000 (0%)]\\tLoss: 99.354678\\n\",\n",
    "            \"Train Epoch: 16 [10000/60000 (17%)]\\tLoss: 105.740625\\n\",\n",
    "            \"Train Epoch: 16 [20000/60000 (33%)]\\tLoss: 99.504209\\n\",\n",
    "            \"Train Epoch: 16 [30000/60000 (50%)]\\tLoss: 101.828975\\n\",\n",
    "            \"Train Epoch: 16 [40000/60000 (67%)]\\tLoss: 104.617754\\n\",\n",
    "            \"Train Epoch: 16 [50000/60000 (83%)]\\tLoss: 102.602461\\n\",\n",
    "            \"====> Epoch: 16 Average loss: 102.4024\\n\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stderr\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \" 57%|█████▋    | 17/30 [07:07<04:47, 22.14s/it]\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stdout\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"\\n\",\n",
    "            \"\\n\",\n",
    "            \"Train Epoch: 17 [0/60000 (0%)]\\tLoss: 103.964189\\n\",\n",
    "            \"Train Epoch: 17 [10000/60000 (17%)]\\tLoss: 106.956523\\n\",\n",
    "            \"Train Epoch: 17 [20000/60000 (33%)]\\tLoss: 100.923945\\n\",\n",
    "            \"Train Epoch: 17 [30000/60000 (50%)]\\tLoss: 99.638047\\n\",\n",
    "            \"Train Epoch: 17 [40000/60000 (67%)]\\tLoss: 105.895127\\n\",\n",
    "            \"Train Epoch: 17 [50000/60000 (83%)]\\tLoss: 101.659795\\n\",\n",
    "            \"====> Epoch: 17 Average loss: 102.1181\\n\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stderr\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \" 60%|██████    | 18/30 [07:30<04:27, 22.32s/it]\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stdout\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"\\n\",\n",
    "            \"\\n\",\n",
    "            \"Train Epoch: 18 [0/60000 (0%)]\\tLoss: 105.758018\\n\",\n",
    "            \"Train Epoch: 18 [10000/60000 (17%)]\\tLoss: 102.432373\\n\",\n",
    "            \"Train Epoch: 18 [20000/60000 (33%)]\\tLoss: 103.837676\\n\",\n",
    "            \"Train Epoch: 18 [30000/60000 (50%)]\\tLoss: 105.240195\\n\",\n",
    "            \"Train Epoch: 18 [40000/60000 (67%)]\\tLoss: 102.560361\\n\",\n",
    "            \"Train Epoch: 18 [50000/60000 (83%)]\\tLoss: 100.728242\\n\",\n",
    "            \"====> Epoch: 18 Average loss: 101.9874\\n\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stderr\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \" 63%|██████▎   | 19/30 [07:52<04:03, 22.14s/it]\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stdout\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"\\n\",\n",
    "            \"\\n\",\n",
    "            \"Train Epoch: 19 [0/60000 (0%)]\\tLoss: 99.499189\\n\",\n",
    "            \"Train Epoch: 19 [10000/60000 (17%)]\\tLoss: 103.872676\\n\",\n",
    "            \"Train Epoch: 19 [20000/60000 (33%)]\\tLoss: 104.278584\\n\",\n",
    "            \"Train Epoch: 19 [30000/60000 (50%)]\\tLoss: 102.210625\\n\",\n",
    "            \"Train Epoch: 19 [40000/60000 (67%)]\\tLoss: 95.183525\\n\",\n",
    "            \"Train Epoch: 19 [50000/60000 (83%)]\\tLoss: 103.128242\\n\",\n",
    "            \"====> Epoch: 19 Average loss: 101.7900\\n\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stderr\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \" 67%|██████▋   | 20/30 [08:14<03:42, 22.26s/it]\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stdout\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"\\n\",\n",
    "            \"\\n\",\n",
    "            \"Train Epoch: 20 [0/60000 (0%)]\\tLoss: 106.172080\\n\",\n",
    "            \"Train Epoch: 20 [10000/60000 (17%)]\\tLoss: 97.980537\\n\",\n",
    "            \"Train Epoch: 20 [20000/60000 (33%)]\\tLoss: 102.122207\\n\",\n",
    "            \"Train Epoch: 20 [30000/60000 (50%)]\\tLoss: 99.069766\\n\",\n",
    "            \"Train Epoch: 20 [40000/60000 (67%)]\\tLoss: 102.640684\\n\",\n",
    "            \"Train Epoch: 20 [50000/60000 (83%)]\\tLoss: 102.569521\\n\",\n",
    "            \"====> Epoch: 20 Average loss: 101.5309\\n\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stderr\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \" 70%|███████   | 21/30 [08:37<03:22, 22.45s/it]\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stdout\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"\\n\",\n",
    "            \"\\n\",\n",
    "            \"Train Epoch: 21 [0/60000 (0%)]\\tLoss: 106.950605\\n\",\n",
    "            \"Train Epoch: 21 [10000/60000 (17%)]\\tLoss: 103.145684\\n\",\n",
    "            \"Train Epoch: 21 [20000/60000 (33%)]\\tLoss: 102.567236\\n\",\n",
    "            \"Train Epoch: 21 [30000/60000 (50%)]\\tLoss: 97.868984\\n\",\n",
    "            \"Train Epoch: 21 [40000/60000 (67%)]\\tLoss: 102.005576\\n\",\n",
    "            \"Train Epoch: 21 [50000/60000 (83%)]\\tLoss: 106.501875\\n\",\n",
    "            \"====> Epoch: 21 Average loss: 101.4325\\n\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stderr\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \" 73%|███████▎  | 22/30 [09:00<02:59, 22.42s/it]\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stdout\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"\\n\",\n",
    "            \"\\n\",\n",
    "            \"Train Epoch: 22 [0/60000 (0%)]\\tLoss: 98.520498\\n\",\n",
    "            \"Train Epoch: 22 [10000/60000 (17%)]\\tLoss: 102.110361\\n\",\n",
    "            \"Train Epoch: 22 [20000/60000 (33%)]\\tLoss: 102.820068\\n\",\n",
    "            \"Train Epoch: 22 [30000/60000 (50%)]\\tLoss: 99.073027\\n\",\n",
    "            \"Train Epoch: 22 [40000/60000 (67%)]\\tLoss: 102.781504\\n\",\n",
    "            \"Train Epoch: 22 [50000/60000 (83%)]\\tLoss: 100.287266\\n\",\n",
    "            \"====> Epoch: 22 Average loss: 101.2391\\n\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stderr\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \" 77%|███████▋  | 23/30 [09:24<02:41, 23.09s/it]\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stdout\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"\\n\",\n",
    "            \"\\n\",\n",
    "            \"Train Epoch: 23 [0/60000 (0%)]\\tLoss: 103.314619\\n\",\n",
    "            \"Train Epoch: 23 [10000/60000 (17%)]\\tLoss: 98.415312\\n\",\n",
    "            \"Train Epoch: 23 [20000/60000 (33%)]\\tLoss: 101.936797\\n\",\n",
    "            \"Train Epoch: 23 [30000/60000 (50%)]\\tLoss: 103.535527\\n\",\n",
    "            \"Train Epoch: 23 [40000/60000 (67%)]\\tLoss: 99.841592\\n\",\n",
    "            \"Train Epoch: 23 [50000/60000 (83%)]\\tLoss: 101.279355\\n\",\n",
    "            \"====> Epoch: 23 Average loss: 101.1207\\n\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stderr\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \" 80%|████████  | 24/30 [10:00<02:41, 26.88s/it]\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stdout\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"\\n\",\n",
    "            \"\\n\",\n",
    "            \"Train Epoch: 24 [0/60000 (0%)]\\tLoss: 98.831211\\n\",\n",
    "            \"Train Epoch: 24 [10000/60000 (17%)]\\tLoss: 99.431797\\n\",\n",
    "            \"Train Epoch: 24 [20000/60000 (33%)]\\tLoss: 102.016113\\n\",\n",
    "            \"Train Epoch: 24 [30000/60000 (50%)]\\tLoss: 107.593291\\n\",\n",
    "            \"Train Epoch: 24 [40000/60000 (67%)]\\tLoss: 97.494277\\n\",\n",
    "            \"Train Epoch: 24 [50000/60000 (83%)]\\tLoss: 100.006016\\n\",\n",
    "            \"====> Epoch: 24 Average loss: 100.9956\\n\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stderr\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \" 83%|████████▎ | 25/30 [10:28<02:15, 27.18s/it]\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stdout\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"\\n\",\n",
    "            \"\\n\",\n",
    "            \"Train Epoch: 25 [0/60000 (0%)]\\tLoss: 100.961348\\n\",\n",
    "            \"Train Epoch: 25 [10000/60000 (17%)]\\tLoss: 102.456904\\n\",\n",
    "            \"Train Epoch: 25 [20000/60000 (33%)]\\tLoss: 102.087725\\n\",\n",
    "            \"Train Epoch: 25 [30000/60000 (50%)]\\tLoss: 101.062266\\n\",\n",
    "            \"Train Epoch: 25 [40000/60000 (67%)]\\tLoss: 98.963867\\n\",\n",
    "            \"Train Epoch: 25 [50000/60000 (83%)]\\tLoss: 101.273379\\n\",\n",
    "            \"====> Epoch: 25 Average loss: 100.8900\\n\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stderr\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \" 87%|████████▋ | 26/30 [10:55<01:48, 27.03s/it]\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stdout\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"\\n\",\n",
    "            \"\\n\",\n",
    "            \"Train Epoch: 26 [0/60000 (0%)]\\tLoss: 100.294141\\n\",\n",
    "            \"Train Epoch: 26 [10000/60000 (17%)]\\tLoss: 96.490518\\n\",\n",
    "            \"Train Epoch: 26 [20000/60000 (33%)]\\tLoss: 99.747197\\n\",\n",
    "            \"Train Epoch: 26 [30000/60000 (50%)]\\tLoss: 103.566826\\n\",\n",
    "            \"Train Epoch: 26 [40000/60000 (67%)]\\tLoss: 104.253789\\n\",\n",
    "            \"Train Epoch: 26 [50000/60000 (83%)]\\tLoss: 100.758867\\n\",\n",
    "            \"====> Epoch: 26 Average loss: 100.7645\\n\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stderr\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \" 90%|█████████ | 27/30 [11:27<01:25, 28.54s/it]\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stdout\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"\\n\",\n",
    "            \"\\n\",\n",
    "            \"Train Epoch: 27 [0/60000 (0%)]\\tLoss: 99.568330\\n\",\n",
    "            \"Train Epoch: 27 [10000/60000 (17%)]\\tLoss: 98.160859\\n\",\n",
    "            \"Train Epoch: 27 [20000/60000 (33%)]\\tLoss: 100.008027\\n\",\n",
    "            \"Train Epoch: 27 [30000/60000 (50%)]\\tLoss: 98.713887\\n\",\n",
    "            \"Train Epoch: 27 [40000/60000 (67%)]\\tLoss: 100.013105\\n\",\n",
    "            \"Train Epoch: 27 [50000/60000 (83%)]\\tLoss: 100.127930\\n\",\n",
    "            \"====> Epoch: 27 Average loss: 100.5838\\n\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stderr\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \" 93%|█████████▎| 28/30 [11:56<00:57, 28.81s/it]\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stdout\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"\\n\",\n",
    "            \"\\n\",\n",
    "            \"Train Epoch: 28 [0/60000 (0%)]\\tLoss: 97.442754\\n\",\n",
    "            \"Train Epoch: 28 [10000/60000 (17%)]\\tLoss: 98.910566\\n\",\n",
    "            \"Train Epoch: 28 [20000/60000 (33%)]\\tLoss: 99.684492\\n\",\n",
    "            \"Train Epoch: 28 [30000/60000 (50%)]\\tLoss: 97.845098\\n\",\n",
    "            \"Train Epoch: 28 [40000/60000 (67%)]\\tLoss: 100.194414\\n\",\n",
    "            \"Train Epoch: 28 [50000/60000 (83%)]\\tLoss: 103.870166\\n\",\n",
    "            \"====> Epoch: 28 Average loss: 100.5474\\n\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stderr\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \" 97%|█████████▋| 29/30 [12:26<00:29, 29.08s/it]\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stdout\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"\\n\",\n",
    "            \"\\n\",\n",
    "            \"Train Epoch: 29 [0/60000 (0%)]\\tLoss: 99.117910\\n\",\n",
    "            \"Train Epoch: 29 [10000/60000 (17%)]\\tLoss: 104.647188\\n\",\n",
    "            \"Train Epoch: 29 [20000/60000 (33%)]\\tLoss: 101.190098\\n\",\n",
    "            \"Train Epoch: 29 [30000/60000 (50%)]\\tLoss: 103.266348\\n\",\n",
    "            \"Train Epoch: 29 [40000/60000 (67%)]\\tLoss: 100.361055\\n\",\n",
    "            \"Train Epoch: 29 [50000/60000 (83%)]\\tLoss: 98.973789\\n\",\n",
    "            \"====> Epoch: 29 Average loss: 100.3897\\n\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stderr\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"100%|██████████| 30/30 [13:01<00:00, 26.05s/it]\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stdout\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"\\n\",\n",
    "            \"\\n\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"stderr\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"\\n\"\n",
    "          ]\n",
    "        }\n",
    "      ],\n",
    "      \"source\": [\n",
    "        \"# 8 model train and test\\n\",\n",
    "        \"from tqdm.auto import tqdm\\n\",\n",
    "        \"\\n\",\n",
    "        \"for epoch in tqdm(range(0, epochs)):\\n\",\n",
    "        \"    train(epoch, model, train_loader, optimizer)\\n\",\n",
    "        \"    test(epoch, model, test_loader)\\n\",\n",
    "        \"    print(\\\"\\\\n\\\")\\n\",\n",
    "        \"writer.close()\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": 51,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# tensorboard --logdir scalar/\\n\",\n",
    "        \"def generate_samples(model, num_samples=8):\\n\",\n",
    "        \"    model.eval()\\n\",\n",
    "        \"    with torch.no_grad():\\n\",\n",
    "        \"        # 표준 정규 분포에서 잠재 벡터 샘플링\\n\",\n",
    "        \"        z = torch.randn(num_samples, latent_dim).to(device)\\n\",\n",
    "        \"        # 디코더로 이미지 생성\\n\",\n",
    "        \"        generated_images = model.Decoder(z).view(-1, 1, 28, 28)\\n\",\n",
    "        \"        # TensorBoard에 시각화\\n\",\n",
    "        \"        grid = torchvision.utils.make_grid(generated_images.cpu(), nrow=num_samples)\\n\",\n",
    "        \"        writer.add_image(\\\"Generated Images from Latent Space\\\", grid, epoch)\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"generate_samples(model, num_samples=80)\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": 55,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"name\": \"stderr\",\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"NaN or Inf found in input tensor.\\n\"\n",
    "          ]\n",
    "        }\n",
    "      ],\n",
    "      \"source\": [\n",
    "        \"from scipy import interpolate\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"def interpolate_images(model, x1, x2, n_steps=10):\\n\",\n",
    "        \"    model.eval()\\n\",\n",
    "        \"    with torch.no_grad():\\n\",\n",
    "        \"        x1, x2 = x1.view(1, -1).float().to(device), x2.view(1, -1).float().to(device)\\n\",\n",
    "        \"        mean1, log_var1 = model.Encoder(x1)\\n\",\n",
    "        \"        mean2, log_var2 = model.Encoder(x2)\\n\",\n",
    "        \"        z1 = model.reparameterization(mean1, torch.exp(0.5 * log_var1))\\n\",\n",
    "        \"        z2 = model.reparameterization(mean2, torch.exp(0.5 * log_var2))\\n\",\n",
    "        \"        interpolated_images = []\\n\",\n",
    "        \"        for alpha in torch.linspace(0, 1, n_steps):\\n\",\n",
    "        \"            z = alpha * z1 + (1 - alpha) * z2\\n\",\n",
    "        \"            img = model.Decoder(z).view(1, 1, 28, 28)\\n\",\n",
    "        \"            interpolated_images.append(img)\\n\",\n",
    "        \"        grid = torchvision.utils.make_grid(torch.cat(interpolated_images), nrow=n_steps)\\n\",\n",
    "        \"        writer.add_image(\\\"Interpolated Images\\\", grid, 0)\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"interpolate_images(\\n\",\n",
    "        \"    model=model, x1=train_dataset.data[0], x2=train_dataset.data[3], n_steps=100\\n\",\n",
    "        \")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": 49,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"def perturb_latent(model, x, num_variations=5, noise_scale=0.1):\\n\",\n",
    "        \"    model.eval()\\n\",\n",
    "        \"    with torch.no_grad():\\n\",\n",
    "        \"        x = x.view(1, -1).float().to(device)\\n\",\n",
    "        \"        mean, log_var = model.Encoder(x)\\n\",\n",
    "        \"        z = model.reparameterization(mean, torch.exp(0.5 * log_var))\\n\",\n",
    "        \"        variations = []\\n\",\n",
    "        \"        for _ in range(num_variations):\\n\",\n",
    "        \"            z_perturbed = z + noise_scale * torch.randn_like(z).to(device)\\n\",\n",
    "        \"            img = model.Decoder(z_perturbed).view(1, 1, 28, 28)\\n\",\n",
    "        \"            variations.append(img)\\n\",\n",
    "        \"        grid = torchvision.utils.make_grid(torch.cat(variations), nrow=num_variations)\\n\",\n",
    "        \"        writer.add_image(\\\"Perturbed Variations\\\", grid, 0)\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"perturb_latent(model=model, x=train_dataset.data[0], num_variations=10, noise_scale=0.1)\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"def manipulate_latent(model, x, dim_idx=0, scale=1.0):\\n\",\n",
    "        \"    model.eval()\\n\",\n",
    "        \"    with torch.no_grad():\\n\",\n",
    "        \"        x = x.view(1, -1).float().to(device)\\n\",\n",
    "        \"        mean, log_var = model.Encoder(x)\\n\",\n",
    "        \"        z = model.reparameterization(mean, torch.exp(0.5 * log_var))\\n\",\n",
    "        \"        z_modified = z.clone()\\n\",\n",
    "        \"        z_modified[0, dim_idx] += scale  # 특정 차원 조작\\n\",\n",
    "        \"        img = model.Decoder(z_modified).view(1, 1, 28, 28)\\n\",\n",
    "        \"        grid = torchvision.utils.make_grid(img)\\n\",\n",
    "        \"        writer.add_image(f\\\"Manipulated Dim {dim_idx}\\\", grid, 0)\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"manipulate_latent(model=model, x=train_dataset.data[0], dim_idx=0, scale=1.0)\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": []\n",
    "    }\n",
    "  ],\n",
    "  \"metadata\": {\n",
    "    \"accelerator\": \"GPU\",\n",
    "    \"colab\": {\n",
    "      \"gpuType\": \"T4\",\n",
    "      \"provenance\": []\n",
    "    },\n",
    "    \"kernelspec\": {\n",
    "      \"display_name\": \"Python 3\",\n",
    "      \"name\": \"python3\"\n",
    "    },\n",
    "    \"language_info\": {\n",
    "      \"codemirror_mode\": {\n",
    "        \"name\": \"ipython\",\n",
    "        \"version\": 3\n",
    "      },\n",
    "      \"file_extension\": \".py\",\n",
    "      \"mimetype\": \"text/x-python\",\n",
    "      \"name\": \"python\",\n",
    "      \"nbconvert_exporter\": \"python\",\n",
    "      \"pygments_lexer\": \"ipython3\",\n",
    "      \"version\": \"3.10.12\"\n",
    "    }\n",
    "  },\n",
    "  \"nbformat\": 4,\n",
    "  \"nbformat_minor\": 0\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
